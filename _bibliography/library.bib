Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Lammich2017,
author = {Lammich, Peter and Sefidgar, S. Reza},
doi = {10.1007/s10817-017-9442-4},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Lammich, Sefidgar - Formalizing Network Flow Algorithms A Refinement Approach in IsabelleHOL.pdf:pdf},
issn = {0168-7433},
journal = {Journal of Automated Reasoning},
keywords = {Maximum flow problem,Edmonds–Karp algorithm,Push–r,edmonds,formal verification,hol,isabelle,karp algorithm,maximum flow problem,push,relabel algorithm,stepwise refinement},
publisher = {Springer Netherlands},
title = {{Formalizing Network Flow Algorithms: A Refinement Approach in Isabelle/HOL}},
url = {http://link.springer.com/10.1007/s10817-017-9442-4},
year = {2017}
}
@article{Stefanowski2001,
abstract = {The rough set theory, based on the original definition of the indiscernibility relation, is not useful for analysing incomplete information tables where some values of attributes are unknown. In this paper we distinguish two different semantics for incomplete information: the "missing value" semantics and the "absent value" semantics. The already known approaches, e.g. based on the tolerance relations, deal with the missing value case. We introduce two generalisations of the rough sets theory to handle these situations. The first generalisation introduces the use of a non symmetric similarity relation in order to formalise the idea of absent value semantics. The second proposal is based on the use of valued tolerance relations. A logical analysis and the computational experiments show that for the valued tolerance approach it is possible to obtain more informative approximations and decision rules than using the approach based on the simple tolerance relation.},
author = {Stefanowski, J. and Tsouki{\`{a}}s, A.},
doi = {10.1111/0824-7935.00162},
file = {:Users/jonaprieto/Desktop/Mendeley/2001 - Stefanowski, Tsouki{\`{a}}s - Incomplete information tables and rough classification.pdf:pdf},
issn = {08247935},
journal = {Computational Intelligence},
keywords = {Decision rules,Fuzzy sets,Incomplete information,Rough sets,Similarity relation,Valued tolerance relation},
number = {3},
pages = {545--566},
title = {{Incomplete information tables and rough classification}},
volume = {17},
year = {2001}
}
@article{Luo2014,
author = {Luo, Chuan and Li, Tianrui},
doi = {10.1007/978-3-319-08644-6_13},
file = {:Users/jonaprieto/Desktop/Mendeley/2014 - Luo, Li - Incremental three-way decisions with incomplete information.pdf:pdf},
isbn = {9783319086439},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {incomplete decision system,incremental updating,three-way decisions},
pages = {128--135},
title = {{Incremental three-way decisions with incomplete information}},
volume = {8536 LNAI},
year = {2014}
}
@article{Liu2015,
abstract = {As a natural extension of three-way decisions with incomplete information, this paper provides a novel three-way decision model based on incomplete information system. First, we define a new relation to describe the similarity degree of incomplete information. Then, in view of the missing values presented in incomplete information system, we utilize interval number to acquire the loss function. A hybrid information table which consist both of the incomplete information and loss function, is used to deal with the new three-way decision model. The key steps and algorithm for constructing the integrated three-way decision model are also carefully investigated. An empirical study of medical diagnosis validates the reasonability and effectiveness of our proposed model.},
author = {Liu, Dun and Liang, Decui and Wang, Changchun},
doi = {10.1016/j.knosys.2015.07.036},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Liu, Liang, Wang - A novel three-way decision model based on incomplete information system.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Decision-theoretic rough sets,Hybrid information system,Incomplete information system,Loss function,Three-way decisions},
number = {July},
pages = {32--45},
publisher = {Elsevier B.V.},
title = {{A novel three-way decision model based on incomplete information system}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115003007},
volume = {91},
year = {2015}
}
@incollection{acuna2004treatment,
author = {Acuna, Edgar and Rodriguez, Caroline},
booktitle = {Classification, clustering, and data mining applications},
pages = {639--647},
publisher = {Springer},
title = {{The treatment of missing values and its effect on classifier accuracy}},
year = {2004}
}
@article{Sutcliffe1996,
author = {Sutcliffe, Geoff},
file = {:Users/jonaprieto/Desktop/Mendeley/1996 - Sutcliffe - The Practice of Clausification in Automatic Theorem Proving.pdf:pdf},
keywords = {automatic theorem proving,clauses,computing review categories,resolution},
number = {18},
pages = {57--68},
title = {{The Practice of Clausification in Automatic Theorem Proving}},
year = {1996}
}
@book{Buurlage,
author = {Buurlage, Jan-willem},
file = {:Users/jonaprieto/Desktop/Mendeley/Unknown - Buurlage - Categories and Haskell An introduction to the mathematics behind modern functional programming.pdf:pdf},
title = {{Categories and Haskell: An introduction to the mathematics behind modern functional programming}}
}
@article{hurd2003first,
abstract = {In this paper we evaluate the effectiveness of first-order proof procedures when used as tactics for proving subgoals in a higher-order logic interactive theorem prover. We first motivate why such first-order proof tactics are useful, and then describe the core integrating technology: an `LCF-style' logical kernel for clausal first-order logic. This allows the choice of different logical mappings between higher-order logic and first-order logic to be used depending on the subgoal, and also enables several different first-order proof procedures to cooperate on constructing the proof. This work was carried out using the HOL4 theorem prover; we comment on the ease of transferring the technology to other higher-order logic theorem provers},
author = {Hurd, Joe},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Hurd - 2003 - First-order Proof Tactics In Higher-order Logic Theorem Provers.pdf:pdf},
journal = {Design and Application of Strategies/Tactics in Higher Order Logics, number NASA/CP-2003-212448 in NASA Technical Reports},
pages = {56--68},
title = {{First-order Proof Tactics In Higher-order Logic Theorem Provers}},
url = {http://www.gilith.com/research/papers},
year = {2003}
}
@article{grzymala2003data,
author = {Grzymala-Busse, Jerzy W and Ziarko, Wojciech},
journal = {Data Mining: Opportunities and Challenges},
pages = {142--173},
publisher = {Hershey, PA: Idea Group Publ},
title = {{Data mining based on rough sets}},
volume = {2},
year = {2003}
}
@incollection{bohme2010,
abstract = {The Satisfiability Modulo Theories (SMT) solver Z3 can generate proofs of unsatisfiability. We present independent reconstruction of these proofs in the theorem provers Isabelle/HOL and HOL4 with particular focus on efficiency. Our highly optimized implementations outperform previous LCF-style proof checkers for SMT, often by orders of magnitude. Detailed performance data shows that LCF-style proof reconstruction can be faster than proof search in Z3.},
address = {Berlin, Heidelberg},
author = {B{\"{o}}hme, Sascha and Weber, Tjark},
booktitle = {Interactive Theorem Proving: First International Conference, ITP 2010, Edinburgh, UK, July 11-14, 2010. Proceedings},
doi = {10.1007/978-3-642-14052-5_14},
editor = {Kaufmann, Matt and Paulson, Lawrence C.},
file = {:Users/jonaprieto/Desktop/Mendeley/2010 - B{\"{o}}hme, Weber - Fast LCF-Style Proof Reconstruction for Z3.pdf:pdf;:Users/jonaprieto/Desktop/Mendeley/2010 - B{\"{o}}hme, Weber - Fast LCF-Style Proof Reconstruction for Z3(2).pdf:pdf},
isbn = {978-3-642-14052-5},
pages = {179--194},
publisher = {Springer Berlin Heidelberg},
title = {{Fast LCF-Style Proof Reconstruction for Z3}},
year = {2010}
}
@article{Clark2016,
author = {Clark, Patrick G and Gao, Cheng and Grzymala-busse, Jerzy W},
doi = {10.1007/978-3-319-47160-0},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Clark, Gao, Grzymala-busse - 2016 - Rule Set Complexity for Incomplete Data Sets with Many Attribute-Concept Values and Do Not Care Cond.pdf:pdf},
isbn = {978-3-319-47159-4},
keywords = {attribute-concept values,conditions,do not care,incomplete data,mlem2 rule induction algo-,probabilistic approximations},
pages = {65--74},
title = {{Rule Set Complexity for Incomplete Data Sets with Many Attribute-Concept Values and Do Not Care Conditions}},
url = {http://link.springer.com/10.1007/978-3-319-47160-0},
volume = {9920},
year = {2016}
}
@article{Ekici2017,
abstract = {This paper describes SMTCoq, a plug-in for the integration of external solvers into the Coq proof assistant. Based on a checker for generic first-order proof certificates fully implemented and proved correct in Coq, SMTCoq offers facilities to check answers from external SAT and SMT solvers and to increase Coq's automation using such solvers, all in a safeway. The current version supports proof certificates produced by the SAT solver ZChaff, for propositional logic, and the SMT solvers veriT and CVC4, for the quantifier-free fragment of the combined theory of fixed-size bit vectors, functional arrays with extensionality, linear integer arithmetic, and uninterpreted function symbols.},
author = {Ekici, B. and Mebsout, A. and Tinelli, C. and Keller, C. and Katz, G.},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Ekici et al. - SMTCoq A plug-in for integrating SMT solvers into Coq.pdf:pdf;:Users/jonaprieto/Desktop/Mendeley/2017 - Ekici et al. - SMTCoq A plug-in for integrating SMT solvers into Coq(2).pdf:pdf},
journal = {stanford.edu},
title = {{SMTCoq: A plug-in for integrating SMT solvers into Coq}},
url = {http://web.stanford.edu/{~}guyk/pub/CAV2017{\_}C.pdf},
year = {2017}
}
@article{Bezem2002,
author = {Bezem, Marc and Hendriks, Dimitri and {De Nivelle}, Hans},
doi = {10.1023/A:1021939521172},
file = {:Users/jonaprieto/Desktop/Mendeley/2002 - Bezem, Hendriks, De Nivelle - Automated proof construction in type theory using resolution.pdf:pdf},
isbn = {3540676643},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {Proof construction,Resolution theorem proving,Type theory},
number = {3-4},
pages = {253--275},
title = {{Automated proof construction in type theory using resolution}},
volume = {29},
year = {2002}
}
@incollection{Li2005,
address = {Berlin, Heidelberg},
author = {Li, Dan and Deogun, Jitender and Spaulding, William and Shuart, Bill},
booktitle = {Transactions on Rough Sets IV},
doi = {10.1007/11574798_3},
editor = {Peters, James F and Skowron, Andrzej},
isbn = {978-3-540-32016-6},
pages = {37--57},
publisher = {Springer Berlin Heidelberg},
title = {{Dealing with Missing Data: Algorithms Based on Fuzzy Set and Rough Set Theories}},
url = {http://dx.doi.org/10.1007/11574798{\_}3},
year = {2005}
}
@article{nelwamondo2007rough,
author = {Nelwamondo, Fulufhelo Vincent and Marwala, Tshilidzi},
journal = {arXiv preprint arXiv:0704.3635},
title = {{Rough sets computations to impute missing data}},
year = {2007}
}
@book{M,
author = {M, Nicholas J.},
file = {:Users/jonaprieto/Desktop/Mendeley/Unknown - M - Handbook of writing for the mathematical sciences.pdf.pdf:pdf},
title = {{Handbook of writing for the mathematical sciences.pdf}}
}
@article{Ingene1993,
author = {Ingene, Charles A and Editorials, Guest and Editorial, Guest and Commentaries, Guest and Commentary, Guest},
file = {:Users/jonaprieto/Desktop/Mendeley/1993 - Ingene et al. - Manuscript Guidelines.pdf:pdf},
number = {4},
pages = {467--468},
title = {{Manuscript Guidelines}},
volume = {69},
year = {1993}
}
@incollection{foster2011integrating,
abstract = {Agda is a dependently typed functional programming language and a proof assistant in which developing programs and proving their correctness is one activity. We show how this process can be enhanced by integrating external automated theorem provers, provide a prototypical integration of the equational theorem prover Waldmeister, and give examples of how this proof automation works in practice.},
address = {Berlin, Heidelberg},
author = {Foster, Simon and Struth, Georg},
booktitle = {NASA Formal Methods: Third International Symposium, NFM 2011, Pasadena, CA, USA, April 18-20, 2011. Proceedings},
doi = {10.1007/978-3-642-20398-5_10},
file = {:Users/jonaprieto/Desktop/Mendeley/2011 - Foster, Struth - Integrating an Automated Theorem Prover into Agda.pdf:pdf},
isbn = {978-3-642-20398-5},
pages = {116--130},
publisher = {Springer Berlin Heidelberg},
title = {{Integrating an Automated Theorem Prover into Agda}},
year = {2011}
}
@phdthesis{Bauer,
abstract = {In 1998, Thomas Hales published a proof of the Kepler Conjecture, which states that the cubic close packing is the densest possible packing of equally- sized spheres. The proof is by exhaustion on a set of 3050 plane graphs satisfying certain properties, called tame plane graphs. The enumeration of this set has been generated by a computer program, hence the completeness of this enumeration is essential for the proof. In this thesis, we formalize a theory of plane graphs defined as an inductive set in the theorem prover Isabelle/HOL: a plane graph is constructed starting with one face and repeatedly adding new faces. Based on this theory, we formalize one part of the proof of the Kepler Conjecture, the completeness of the enumeration of tame plane graphs},
author = {Bauer, Gertrud Josefine},
file = {:Users/jonaprieto/Desktop/Mendeley/2005 - Bauer - Formalizing Plane Graph Theory – Towards a Formalized Proof of the Kepler Conjecture.pdf:pdf},
title = {{Formalizing Plane Graph Theory – Towards a Formalized Proof of the Kepler Conjecture}},
year = {2005}
}
@article{Wu2009,
author = {Wu, Xindong and Kumar, Vipin and Quinlan, J Ross and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J and Ng, Angus and Liu, Bing and Philip, S Yu and Others},
journal = {Knowledge and information systems},
number = {1},
pages = {1--37},
publisher = {Springer},
title = {{Top 10 algorithms in data mining}},
volume = {14},
year = {2009}
}
@inproceedings{Armand2010,
abstract = {Coq has within its logic a programming language that can be used to replace many deduction steps into a single computation, this is the so-called reflection. In this paper, we present two extensions of the evaluation mechanism that preserve its correctness and make it possible to deal with cpu-intensive tasks such as proof checking of SAT traces.},
author = {Armand, Michael and Gr{\'{e}}goire, Benjamin and Spiwack, Arnaud and Th{\'{e}}ry, Laurent},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-14052-5_8},
isbn = {3642140513},
issn = {03029743},
pages = {83--98},
title = {{Extending Coq with imperative features and its application to SAT verification}},
volume = {6172 LNCS},
year = {2010}
}
@book{VanDalen1994,
address = {Berlin, Heidelberg},
annote = {The process of formalization of propositional logic consists of two stages:
(1) present a formal language, (2) specify a procedure for obtaining valid or
true propositions.},
author = {van Dalen, Dirk},
doi = {10.1007/978-3-662-02962-6},
file = {:Users/jonaprieto/Desktop/Mendeley/1994 - van Dalen - Logic and Structure.pdf:pdf},
isbn = {978-3-540-57839-0},
publisher = {Springer Berlin Heidelberg},
series = {Universitext},
title = {{Logic and Structure}},
url = {http://link.springer.com/10.1007/978-3-662-02962-6},
year = {1994}
}
@phdthesis{Klieber2014,
author = {Klieber, William},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Klieber - 2014 - Formal Verification Using Quantified Boolean Formulas (QBF).pdf:pdf},
keywords = {QBF Solvers},
mendeley-tags = {QBF Solvers},
school = {Carnegie Mellon University},
title = {{Formal Verification Using Quantified Boolean Formulas (QBF)}},
url = {http://www.cs.cmu.edu/{~}wklieber/thesis.pdf},
year = {2014}
}
@inproceedings{Sutcliffe-Schulz-Claessen-VanGelder-2006,
author = {Sutcliffe, Geoff and Schulz, Stephan and Claessen, Koen and {Van Gelder}, Allen},
booktitle = {International Joint Conference on Automated Reasoning (IJCAR 2006)},
editor = {Furbach, Ulrich and Shankar, Natarajan},
pages = {67--81},
publisher = {Springer},
title = {{Using the TPTP Language for Writing Derivations and Finite Interpretations}},
volume = {4130},
year = {2006}
}
@article{Gradel2009,
author = {Gr{\"{a}}del, Erich},
file = {:Users/jonaprieto/Desktop/Mendeley/2009 - Gr{\"{a}}del - Complexity Theory.pdf:pdf},
title = {{Complexity Theory}},
year = {2009}
}
@book{Jackson2006,
abstract = {{\{}In Software Abstractions Daniel Jackson introduces a new approach to software design that draws on traditional formal methods but exploits automated tools to find flaws as early as possible. This approach--which Jackson calls "lightweight formal methods" or "agile modeling"--takes from formal specification the idea of a precise and expressive notation based on a tiny core of simple and robust concepts but replaces conventional analysis based on theorem proving with a fully automated analysis that gives designers immediate feedback. Jackson has developed Alloy, a language that captures the essence of software abstractions simply and succinctly, using a minimal toolkit of mathematical notions. The designer can use automated analysis not only to correct errors but also to make models that are more precise and elegant. This approach, Jackson says, can rescue designers from "the tarpit of implementation technologies" and return them to thinking deeply about underlying concepts. Software Abstractions introduces the key elements of the approach: a logic, which provides the building blocks of the language; a language, which adds a small amount of syntax to the logic for structuring descriptions; and an analysis, a form of constraint solving that offers both simulation (generating sample states and executions) and checking (finding counterexamples to claimed properties). The book uses Alloy as a vehicle because of its simplicity and tool support, but the book's lessons are mostly language-independent, and could also be applied in the context of other modeling languages.{\}}},
author = {Jackson, Daniel},
booktitle = {MIT Press},
file = {:Users/jonaprieto/Desktop/Mendeley/2006 - Jackson - Software Abstractions.pdf:pdf},
number = {02},
pages = {253},
title = {{Software Abstractions}},
volume = {19},
year = {2006}
}
@misc{Athena,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.437196},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Prieto-Cubides - A Translator Tool for Metis Proofs in Haskell.pdf:pdf},
title = {{A Translator Tool for Metis Proofs in Haskell}},
url = {https://doi.org/10.5281/zenodo.437196},
year = {2017}
}
@misc{Schmitt2001,
author = {Schmitt, Stephan and Lorigo, Lori and Kreitz, Christoph and Nogin, Aleksey},
doi = {10.1007/3-540-45744-5_34},
file = {:Users/jonaprieto/Desktop/Mendeley/2001 - Schmitt et al. - JProver Integrating Connection-Based Theorem Proving into Interactive Proof Assistants.pdf:pdf;:Users/jonaprieto/Desktop/Mendeley/2001 - Schmitt et al. - JProver Integrating Connection-Based Theorem Proving into Interactive Proof Assistants(2).pdf:pdf},
pages = {421--426},
publisher = {Springer, Berlin, Heidelberg},
title = {{JProver: Integrating Connection-Based Theorem Proving into Interactive Proof Assistants}},
url = {http://link.springer.com/10.1007/3-540-45744-5{\_}34},
year = {2001}
}
@misc{coqteam,
author = {Team, The Coq Developement},
edition = {Version 8.},
title = {{The Coq Proof Assistant. Reference Manual}},
url = {https://coq.inria.fr/distrib/current/files/Reference-Manual.pdf},
year = {2015}
}
@book{Tucker2004,
author = {Tucker, Allen B.},
file = {:Users/jonaprieto/Desktop/Mendeley/2004 - Tucker - Computer Science Handbook.pdf:pdf},
isbn = {158488360X},
number = {March},
title = {{Computer Science Handbook}},
year = {2004}
}
@article{Liu2015,
abstract = {As a natural extension of three-way decisions with incomplete information, this paper provides a novel three-way decision model based on incomplete information system. First, we define a new relation to describe the similarity degree of incomplete information. Then, in view of the missing values presented in incomplete information system, we utilize interval number to acquire the loss function. A hybrid information table which consist both of the incomplete information and loss function, is used to deal with the new three-way decision model. The key steps and algorithm for constructing the integrated three-way decision model are also carefully investigated. An empirical study of medical diagnosis validates the reasonability and effectiveness of our proposed model.},
author = {Liu, Dun and Liang, Decui and Wang, Changchun},
doi = {10.1016/j.knosys.2015.07.036},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Liu, Liang, Wang - A novel three-way decision model based on incomplete information system.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Decision-theoretic rough sets,Hybrid information system,Incomplete information system,Loss function,Three-way decisions},
number = {July},
pages = {32--45},
publisher = {Elsevier B.V.},
title = {{A novel three-way decision model based on incomplete information system}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115003007},
volume = {91},
year = {2015}
}
@inproceedings{hurlin07practical,
address = {Bremen, Germany},
author = {Hurlin, Cl{\'{e}}ment and Chaieb, Amine and Fontaine, Pascal and Merz, Stephan and Weber, Tjark},
booktitle = {Proceedings of the Isabelle Workshop 2007},
editor = {Dixon, Lucas and Johansson, Moa},
file = {:Users/jonaprieto/Desktop/Mendeley/2007 - Hurlin et al. - Practical Proof Reconstruction for First-Order Logic and Set-Theoretical Constructions.pdf:pdf},
pages = {2--13},
title = {{Practical Proof Reconstruction for First-Order Logic and Set-Theoretical Constructions}},
year = {2007}
}
@inproceedings{Benzmuller2008,
abstract = {LEO-II is a standalone, resolution-based higher-order theorem prover designed for effective cooperation with specialist provers for natural fragments of higher-order logic. At present LEO-II can cooperate with the first-order automated theorem provers E, SPASS, and Vampire. The improved performance of LEO-II, especially in comparison to its predecessor LEO, is due to several novel features including the exploitation of term sharing and term indexing techniques, support for primitive equality reasoning, and improved heuristics at the calculus level. LEO-II is implemented in Objective Caml and its problem representation language is the new TPTP THF language.},
author = {Benzm{\"{u}}ller, Christoph and Paulson, Lawrence C. and Theiss, Frank and Fietzke, Arnaud},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-71070-7_14},
isbn = {3540710698},
issn = {03029743},
pages = {162--170},
title = {{LEO-II - A cooperative automatic theorem prover for classical higher-order logic (system description)}},
volume = {5195 LNAI},
year = {2008}
}
@article{Narayan2008,
abstract = {Call graphs depict the static, caller-callee relation between "functions " in a program. With most source/target languages supporting functions as the primitive unit of composition, call graphs naturally form the fundamental control flow representation available to understand/develop software. They are also the substrate on which various inter- procedural analyses are performed and are integral part of program comprehension/testing. Given their universality and usefulness, it is imperative to ask if call graphs exhibit any intrinsic graph theoretic features - across versions, program domains and source languages. This work is an attempt to answer these questions: we present and investigate a set of meaningful graph measures that help us understand call graphs better; we establish how these measures correlate, if any, across different languages and program domains; we also assess the overall, language independent software quality by suitably interpreting these measures.},
archivePrefix = {arXiv},
arxivId = {0803.4025},
author = {Narayan, Ganesh and Gopinath, K. and Sridhar, V.},
doi = {10.1109/TASE.2008.40},
eprint = {0803.4025},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Narayan, Gopinath, Sridhar - 2008 - Structure and interpretation of computer programs.pdf:pdf},
isbn = {9780769532493},
issn = {0018-9219},
journal = {Proceedings - 2nd IFIP/IEEE International Symposium on Theoretical Aspects of Software Engineering, TASE 2008},
pages = {73--80},
pmid = {3524258},
title = {{Structure and interpretation of computer programs}},
year = {2008}
}
@article{pawlak1998rough,
author = {Pawlak, Zdzislaw},
journal = {Cybernetics {\&} Systems},
number = {7},
pages = {661--688},
publisher = {Taylor {\&} Francis},
title = {{Rough set theory and its applications to data analysis}},
volume = {29},
year = {1998}
}
@book{nipkow2002isabelle,
author = {Nipkow, Tobias and Paulson, Lawrence C. and Wenzel, Markus},
publisher = {Springer Science {\&} Business Media},
title = {{Isabelle/HOL: A Proof Assistant for Higher-order Logic}},
volume = {2283},
year = {2002}
}
@misc{Pitts1991,
author = {Pitts, Andrew},
file = {:Users/jonaprieto/Desktop/Mendeley/1991 - Pitts - Notes on Categorical Logic.pdf:pdf},
title = {{Notes on Categorical Logic}},
year = {1991}
}
@incollection{Gantayat2014,
address = {Cham},
author = {Gantayat, S S and Misra, Ashok and Panda, B S},
booktitle = {Proceedings of the International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA) 2013},
doi = {10.1007/978-3-319-02931-3_45},
editor = {Satapathy, Suresh Chandra and Udgata, Siba K and Biswal, Bhabendra Narayan},
isbn = {978-3-319-02931-3},
pages = {401--408},
publisher = {Springer International Publishing},
title = {{A Study of Incomplete Data -- A Review}},
url = {http://dx.doi.org/10.1007/978-3-319-02931-3{\_}45},
year = {2014}
}
@article{gediga2003maximum,
author = {Gediga, G{\"{u}}nther and D{\"{u}}ntsch, Ivo},
journal = {Artificial Intelligence Review},
number = {1},
pages = {93--107},
publisher = {Springer},
title = {{Maximum consistency of incomplete data via non-invasive imputation}},
volume = {19},
year = {2003}
}
@article{Norell2009,
abstract = {Dependently typed languages have for a long time been used to describe proofs about programs. Traditionally, dependent types are used mostly for stating and proving the properties of the programs and not in defining the programs themselves. An impressive example is the certified compiler by Leroy (2006) implemented and proved correct in Coq (Bertot and Cast{\'{e}}ran 2004). Recently there has been an increased interest in dependently typed programming, where the aim is to write programs that use the dependent type system to a much higher degree. In this way a lot of the properties that were previously proved separately can be integrated in the type of the program, in many cases adding little or no complexity to the definition of the program. New languages, such as Epigram (McBride and McKinna 2004), are being designed, and existing languages are being extended with new features to accomodate these ideas, for instance the work on dependently typed programming in Coq by Sozeau (2007). This talk gives an overview of the Agda programming language (Norell 2007), whose main focus is on dependently typed programming. Agda provides a rich set of inductive types with a powerful mechanism for pattern matching, allowing dependently typed programs to be written with minimal fuss. To read about programming in Agda, see the lecture notes from the Advanced Functional Programming summer school (Norell 2008) and the work by Oury and Swierstra (2008). In the talk a number of examples of interesting dependently typed programs chosen from the domain of programming language implementation are presented as they are implemented in Agda.},
author = {Norell, Ulf},
doi = {10.1007/978-3-642-04652-0_5},
file = {:Users/jonaprieto/Desktop/Mendeley/2009 - Norell - Dependently typed programming in agda.pdf:pdf},
isbn = {3642046517},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {Agda 2},
pages = {230--266},
title = {{Dependently typed programming in agda}},
volume = {5832 LNCS},
year = {2009}
}
@inproceedings{grzymala2004characteristic,
author = {Grzyma{\l}a-Busse, Jerzy W},
booktitle = {Rough Sets and Current Trends in Computing},
organization = {Springer},
pages = {244--253},
title = {{Characteristic relations for incomplete data: A generalization of the indiscernibility relation}},
year = {2004}
}
@article{Dufourd2009,
author = {Dufourd, Jean Fran{\c{c}}ois},
doi = {10.1007/s10817-009-9117-x},
file = {:Users/jonaprieto/Desktop/Mendeley/2009 - Dufourd - An intuitionistic proof of a discrete form of the Jordan curve theorem formalized in coq with combinatorial hypermaps.pdf:pdf},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {Combinatorial hypermaps,Computational topology,Computer-aided proofs,Coq system,Discrete Jordan Curve Theorem,Formal specifications,Planar subdivisions},
number = {1},
pages = {19--51},
title = {{An intuitionistic proof of a discrete form of the Jordan curve theorem formalized in coq with combinatorial hypermaps}},
volume = {43},
year = {2009}
}
@article{nelwamondo2007missing,
author = {Nelwamondo, Fulufhelo V and Mohamed, Shakir and Marwala, Tshilidzi},
journal = {arXiv preprint arXiv:0704.3474},
title = {{Missing data: A comparison of neural network and expectation maximisation techniques}},
year = {2007}
}
@inproceedings{Weber2006,
abstract = {This paper describes the integration of a leading SAT solver with Isabelle/HOL, a popular interactive theorem prover. The SAT solver generates resolution-style proofs for (instances of) propositional tautologies. These proofs are verified by the theorem prover. The presented approach significantly improves Isabelle's performance on propositional problems, and furthermore exhibits counterexamples for unprovable conjectures. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Weber, Tjark},
booktitle = {Electronic Notes in Theoretical Computer Science},
doi = {10.1016/j.entcs.2005.12.007},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Weber - 2006 - Integrating a SAT solver with an LCF-style theorem prover.pdf:pdf},
issn = {15710661},
keywords = {LCF-style theorem prover,Proof checking,Propositional resolution,SAT solver},
number = {2 SPEC. ISS.},
pages = {67--78},
title = {{Integrating a SAT solver with an LCF-style theorem prover}},
volume = {144},
year = {2006}
}
@incollection{Barrett2011,
abstract = {CVC4 is the latest version of the Cooperating Validity Checker. A joint project of NYU and U Iowa, CVC4 aims to support the useful feature set of CVC3 and SMT-LIBv2 while optimizing the design of the core system architecture and decision procedures to take advantage of recent engineering and algorithmic advances. CVC4 represents a completely new code base; it is a from-scratch rewrite of CVC3, and many subsystems have been completely redesigned. Additional decision procedures for CVC4 are currently under development, but for what it currently achieves, it is a lighter-weight and higher-performing tool than CVC3. We describe the system architecture, subsystems of note, and discuss some applications and continuing work.},
address = {Berlin, Heidelberg},
author = {Barrett, Clark and Conway, Christopher L. and Deters, Morgan and Hadarean, Liana and Jovanovi{\'{c}}, Dejan and King, Tim and Reynolds, Andrew and Tinelli, Cesare},
booktitle = {Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings},
doi = {10.1007/978-3-642-22110-1_14},
editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
isbn = {978-3-642-22110-1},
pages = {171--177},
publisher = {Springer Berlin Heidelberg},
title = {{CVC4}},
year = {2011}
}
@techreport{Fleury2014,
abstract = {Sledgehammer is a powerful interface from Isabelle to automated provers, to discharge subgoals that appear during the interactive proofs. It chooses facts related to this goal and asks some automatic provers to find a proof. The proof can be either reconstructed or just used to extract the relevant lemmas: in both cases the proof is not trusted. We extend the support by adding one first-order prover (Zipperposition), the reconstruction for two higher-order ATPs (Leo-II and Satallax) and an SMT solver veriT. The support of higher-order prover should especially improve Sledgehammer's performance for higher-order goals. Acknowledgement I would like to thank Jasmin Blanchette for the internship about a very interesting subject, and the members of the logic and verification chair for the welcome. Then Simon Cruanes and Pascal Fontaine (developer of Zipperposition and veriT) were very helpful and provided many explanations concerning their provers and bug fixes.},
author = {Fleury, Mathias and Blanchette, Jasmin},
file = {:Users/jonaprieto/Desktop/Mendeley/2014 - Fleury, Blanchette - Translation of Proofs Provided by External Provers More Automatic Prover Support for Isabelle Two Higher-Ord.pdf:pdf},
institution = {Techniche Universit{\"{a}}t M{\"{u}}nchen},
keywords = {Isabelle,Leo-II,Satallax,Sledgehammer,TSTP proof,Zipperposition,higher-order proofs,proof reconstruc-tion,veriT},
pages = {2--0},
title = {{Translation of Proofs Provided by External Provers More Automatic Prover Support for Isabelle: Two Higher-Order Provers and a SMT Solver}},
url = {http://perso.eleves.ens-rennes.fr/{~}mfleur01/documents/Fleury{\_}internship2014.pdf},
year = {2014}
}
@incollection{Otten2008,
address = {Berlin, Heidelberg},
author = {Otten, Jens},
booktitle = {Automated Reasoning},
doi = {10.1007/978-3-540-71070-7_23},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Otten - 2008 - leanCoP 2.0 and ileanCoP 1.2 High Performance Lean Theorem Proving in Classical and Intuitionistic Logic (System Descript.pdf:pdf},
pages = {283--291},
publisher = {Springer Berlin Heidelberg},
title = {{leanCoP 2.0 and ileanCoP 1.2: High Performance Lean Theorem Proving in Classical and Intuitionistic Logic (System Descriptions)}},
url = {http://link.springer.com/10.1007/978-3-540-71070-7{\_}23},
year = {2008}
}
@incollection{Bove2005,
author = {Bove, Ana and Capretta, Venanzio},
doi = {10.1007/11417170_10},
file = {:Users/jonaprieto/Desktop/Mendeley/2005 - Bove, Capretta - Recursive Functions with Higher Order Domains.pdf:pdf},
pages = {116--130},
publisher = {Springer, Berlin, Heidelberg},
title = {{Recursive Functions with Higher Order Domains}},
url = {http://link.springer.com/10.1007/11417170{\_}10},
year = {2005}
}
@book{Salomaa2013,
abstract = {This book presents sequential decision theory from a novel algorithmic information theory perspective. While the former is suited for active agents in known environments, the latter is suited for passive prediction in unknown environments. The book introduces these two well-known but very different ideas and removes the limitations by unifying them to one parameter-free theory of an optimal reinforcement learning agent embedded in an arbitrary unknown environment. Most if not all AI problems can easily be formulated within this theory, which reduces the conceptual problems to pure computational ones. Considered problem classes include sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations to other approaches to AI. One intention of this book is to excite a broader AI audience about abstract algorithmic information theory concepts, and conversely to inform theorists about exciting applications to AI.},
author = {Salomaa, Arto},
doi = {10.1007/978-3-642-16533-7},
file = {:Users/jonaprieto/Desktop/Mendeley/2013 - Salomaa - Public-Key Cryptography (Texts in Theoretical Computer Science. An EATCS Series).pdf:pdf},
isbn = {9783540299523},
pages = {1--288},
title = {{Public-Key Cryptography (Texts in Theoretical Computer Science. An EATCS Series)}},
year = {2013}
}
@article{bai2015novel,
author = {Bai, Xiuling and Zhang, Mingchuan and Wu, Qingtao and Zheng, Ruijuan and Zhao, Haixia and Wei, Wangyang},
journal = {International Journal of Database Theory and Application},
number = {6},
pages = {149--164},
title = {{A Novel Data Filling Algorithm for Incomplete Information System Based on Valued Limited Tolerance Relation}},
volume = {8},
year = {2015}
}
@article{Milewski2014,
author = {Milewski, Bartosz},
file = {:Users/jonaprieto/Desktop/Mendeley/2014 - Milewski - Category Theory for Programmers.pdf:pdf},
title = {{Category Theory for Programmers}},
url = {https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/},
year = {2014}
}
@incollection{kaliszyk2013,
abstract = {PRocH is a proof reconstruction tool that imports in HOL Light proofs produced by ATPs on the recently developed translation of HOL Light and Flyspeck problems to ATP formats. PRocH combines several reconstruction methods in parallel, but the core improvement over previous methods is obtained by re-playing in the HOL logic the detailed inference steps recorded in the ATP (TPTP) proofs, using several internal HOL Light inference methods. These methods range from fast variable matching and more involved rewriting, to full first-order theorem proving using the MESON tactic. The system is described and its performance is evaluated here on a large set of Flyspeck problems.},
address = {Berlin, Heidelberg},
author = {Kaliszyk, Cezary and Urban, Josef},
booktitle = {Automated Deduction -- CADE-24: 24th International Conference on Automated Deduction, Lake Placid, NY, USA, June 9-14, 2013. Proceedings},
doi = {10.1007/978-3-642-38574-2_18},
editor = {Bonacina, Maria Paola},
file = {:Users/jonaprieto/Desktop/Mendeley/2013 - Kaliszyk, Urban - PRocH Proof Reconstruction for HOL Light.pdf:pdf},
isbn = {978-3-642-38574-2},
pages = {267--274},
publisher = {Springer Berlin Heidelberg},
title = {{PRocH: Proof Reconstruction for HOL Light}},
url = {https://doi.org/10.1007/978-3-642-38574-2{\_}18},
year = {2013}
}
@phdthesis{Isaza2014,
author = {Isaza, Juan Pedro Villa},
booktitle = {Eafit.Edu.Co},
file = {:Users/jonaprieto/Desktop/Mendeley/2014 - Isaza - Category Theory Applied to Functional Programming.pdf:pdf},
keywords = {Category Theory,Haskell},
mendeley-tags = {Category Theory,Haskell},
school = {Universidad EAFIT},
title = {{Category Theory Applied to Functional Programming}},
url = {http://www1.eafit.edu.co/asicard/pubs/cain-screen.pdf},
year = {2014}
}
@article{Luo2014,
author = {Luo, Chuan and Li, Tianrui},
doi = {10.1007/978-3-319-08644-6_13},
file = {:Users/jonaprieto/Desktop/Mendeley/2014 - Luo, Li - Incremental three-way decisions with incomplete information.pdf:pdf},
isbn = {9783319086439},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {incomplete decision system,incremental updating,three-way decisions},
pages = {128--135},
title = {{Incremental three-way decisions with incomplete information}},
volume = {8536 LNAI},
year = {2014}
}
@book{paulson1994isabelle,
author = {Paulson, Lawrence C.},
publisher = {Springer Science {\&} Business Media},
title = {{Isabelle: A Generic Theorem Prover}},
volume = {828},
year = {1994}
}
@article{slowinski1989rough,
author = {S{\l}owi{\'{n}}ski, Roman and Stefanowski, Jerzy},
journal = {Mathematical and Computer Modelling},
number = {10},
pages = {1347--1357},
publisher = {Elsevier},
title = {{Rough classification in incomplete information systems}},
volume = {12},
year = {1989}
}
@article{Zeng2015,
abstract = {With the development of the Internet of Things (IoT), more and more hybrid data is being collected by information systems, which are known as Hybrid Information Systems (HIS). Based on a new hybrid distance, novel Gaussian kernel Fuzzy Rough Sets (FRS) for HIS were constructed in our previous study. In real-world applications, with the deepening of cognition and improvements in technology, attribute values in an information system often evolve over time; in particular, there are three cases: when missing values are imputed, error values are corrected, and the values are coarsened or refined. This has posed challenges to developing efficient data analysis algorithms. In this paper, the changing mechanisms of the attribute values and fuzzy equivalence relations in FRS are analyzed. FRS approaches for incrementally updating approximations in HIS are presented. Moreover, two corresponding incremental algorithms are developed. Finally, extensive experiments on eight data sets from the University of California, Irvine (UCI) and an artificial data set show that incremental approaches can effectively improve the performance of updating approximations and not only significantly shorten the computational time, but also increase approximation classification accuracies.},
author = {Zeng, Anping and Li, Tianrui and Luo, Chuan and Hu, Jie},
doi = {10.1109/ICMLC.2015.7340915},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Zeng et al. - Incremental updating fuzzy rough approximations for dynamic hybrid data under the variation of attribute values.pdf:pdf},
isbn = {9781467372213},
issn = {21601348},
journal = {Proceedings - International Conference on Machine Learning and Cybernetics},
keywords = {Fuzzy rough sets,Hybrid information systems,Incremental learning},
pages = {157--162},
publisher = {Elsevier Inc.},
title = {{Incremental updating fuzzy rough approximations for dynamic hybrid data under the variation of attribute values}},
url = {http://dx.doi.org/10.1016/j.ins.2016.07.056},
volume = {1},
year = {2015}
}
@article{Luo2015,
abstract = {Rough set theory is advocated as a framework for conceptualizing and analyzing various types of data, which is a powerful tool for discovering patterns in a given data set through a pair of concepts, namely, upper and lower approximations. Strategic behaviors need to be reinforced continuously under the dynamic decision environment where data in the decision process can change over time. Incremental learning is an effective technique to deal with dynamic learning tasks since it can make full use of previous knowledge. Set-valued data, in which a set of values are associated with an individual, is common in real-world data sets. Motivated by the needs of knowledge updating due to the dynamic variation of criteria values in the set-valued decision system, in this paper, we present the updating properties for dynamic maintenance of approximations when the criteria values in the set-valued decision system evolve with time. Then, two incremental algorithms for computing rough approximations are proposed corresponding to the addition and removal of criteria values, respectively. Experimental results show our incremental algorithms work successfully on datasets from UCI as well as artificial datasets, and achieve better performance than the traditional non-incremental method.},
author = {Luo, Chuan and Li, Tianrui and Chen, Hongmei and Lu, Lixia},
doi = {10.1016/j.ins.2014.12.029},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Luo et al. - 2015 - Fast algorithms for computing rough approximations in set-valued decision systems while updating criteria values.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Incremental learning,Knowledge discovery,Rough set,Set-valued decision systems},
pages = {221--242},
publisher = {Elsevier Inc.},
title = {{Fast algorithms for computing rough approximations in set-valued decision systems while updating criteria values}},
url = {http://dx.doi.org/10.1016/j.ins.2014.12.029},
volume = {299},
year = {2015}
}
@inproceedings{dougherty1995supervised,
author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran and Others},
booktitle = {Machine learning: proceedings of the twelfth international conference},
pages = {194--202},
title = {{Supervised and unsupervised discretization of continuous features}},
volume = {12},
year = {1995}
}
@inproceedings{DeMoura2008,
abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
author = {{De Moura}, Leonardo and Bj{\o}rner, Nikolaj},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-78800-3_24},
isbn = {3540787992},
issn = {03029743},
pages = {337--340},
title = {{Z3: An efficient SMT Solver}},
volume = {4963 LNCS},
year = {2008}
}
@article{Vieira1984,
abstract = {Presents readers' comments on articles appearing in the "Professional Safety Journal." Appeal for a collaboration between the American Society of Safety Engineers and international organizations; Interrelations between risk-taking behavior and accidents.},
author = {Allen, Christopher and Moronuki, Julie},
doi = {10.1080/07366988409450331},
file = {:Users/jonaprieto/.Trash/1984 - Vieira - Reader feedback.pdf:pdf},
isbn = {15241696},
issn = {19361009},
journal = {Edpacs},
number = {7},
pages = {15},
title = {{Haskell programming from principles}},
volume = {11},
year = {1984}
}
@inproceedings{nauck1999learning,
author = {Nauck, Detlef and Kruse, Rudolf},
booktitle = {Neural Information Processing, 1999. Proceedings. ICONIP'99. 6th International Conference on},
organization = {IEEE},
pages = {142--147},
title = {{Learning in neuro-fuzzy systems with symbolic attributes and missing values}},
volume = {1},
year = {1999}
}
@article{appel1959,
author = {Appel, K. I.},
issn = {00224812},
journal = {The Journal of Symbolic Logic},
number = {4},
pages = {306--310},
publisher = {Association for Symbolic Logic},
title = {{Horn Sentences in Identity Theory}},
url = {http://www.jstor.org/stable/2963901},
volume = {24},
year = {1959}
}
@inproceedings{Stump2008,
address = {New York, New York, USA},
author = {Stump, Aaron and Oe, Duckki},
booktitle = {Proceedings of the Joint Workshops of the 6th International Workshop on Satisfiability Modulo Theories and 1st International Workshop on Bit-Precise Reasoning - SMT '08/BPR '08},
doi = {10.1145/1512464.1512470},
file = {:Users/jonaprieto/Desktop/Mendeley/2008 - Stump, Oe - Towards an SMT proof format.pdf:pdf;:Users/jonaprieto/Desktop/Mendeley/2008 - Stump, Oe - Towards an SMT proof format(2).pdf:pdf},
isbn = {9781605584409},
keywords = {logical framework,proof formats},
pages = {27},
publisher = {ACM Press},
title = {{Towards an SMT proof format}},
url = {http://portal.acm.org/citation.cfm?doid=1512464.1512470},
year = {2008}
}
@misc{OnlineATPs,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.398851},
title = {{OnlineATPs: A command-line tool client for the TPTP World.}},
url = {https://doi.org/10.5281/zenodo.398851},
year = {2017}
}
@article{liu2013comparison,
author = {Liu, Yushan and Brown, Steven D},
journal = {Chemometrics and Intelligent Laboratory Systems},
pages = {106--115},
publisher = {Elsevier},
title = {{Comparison of five iterative imputation methods for multivariate classification}},
volume = {120},
year = {2013}
}
@inproceedings{abdella2005use,
author = {Abdella, Mussa and Marwala, Tshilidzi},
booktitle = {Computational Cybernetics, 2005. ICCC 2005. IEEE 3rd International Conference on},
organization = {IEEE},
pages = {207--212},
title = {{The use of genetic algorithms and neural networks to approximate missing data in database}},
year = {2005}
}
@article{Zeng2015a,
abstract = {With the development of the Internet of Things (IoT), more and more hybrid data is being collected by information systems, which are known as Hybrid Information Systems (HIS). Based on a new hybrid distance, novel Gaussian kernel Fuzzy Rough Sets (FRS) for HIS were constructed in our previous study. In real-world applications, with the deepening of cognition and improvements in technology, attribute values in an information system often evolve over time; in particular, there are three cases: when missing values are imputed, error values are corrected, and the values are coarsened or refined. This has posed challenges to developing efficient data analysis algorithms. In this paper, the changing mechanisms of the attribute values and fuzzy equivalence relations in FRS are analyzed. FRS approaches for incrementally updating approximations in HIS are presented. Moreover, two corresponding incremental algorithms are developed. Finally, extensive experiments on eight data sets from the University of California, Irvine (UCI) and an artificial data set show that incremental approaches can effectively improve the performance of updating approximations and not only significantly shorten the computational time, but also increase approximation classification accuracies.},
author = {Zeng, Anping and Li, Tianrui and Luo, Chuan and Hu, Jie},
doi = {10.1109/ICMLC.2015.7340915},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Zeng et al. - Dynamical updating fuzzy rough approximations for hybrid data under the variation of attribute values.pdf:pdf},
isbn = {9781467372213},
issn = {21601348},
journal = {Proceedings - International Conference on Machine Learning and Cybernetics},
keywords = {Fuzzy rough sets,Hybrid information systems,Incremental learning},
pages = {157--162},
publisher = {Elsevier Inc.},
title = {{Dynamical updating fuzzy rough approximations for hybrid data under the variation of attribute values}},
volume = {1},
year = {2017}
}
@book{SIAM2013,
author = {SIAM},
file = {:Users/jonaprieto/Desktop/Mendeley/2013 - SIAM - SIAM Style Manual.pdf:pdf},
isbn = {0644071230},
pages = {122},
title = {{SIAM Style Manual}},
year = {2013}
}
@article{Blackburn2005,
author = {Blackburn, Patrick and Benthem, Johan Van},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Blackburn, Benthem - 2005 - Modal Logic A Semantic Perspective.pdf:pdf},
pages = {1--50},
title = {{Modal Logic: A Semantic Perspective}},
url = {papers2://publication/uuid/CC567B68-4635-4E4C-A795-06E2E0B6C800},
year = {2005}
}
@article{Tammet1997,
abstract = {We give a brief overview of the first-order classical logic component in the Gandalf family of resolution-based automated theorem provers for classical and intuitionistic logics. The main strength of the described version is a sophisticated algorithm for nonunit subsumption. {\textcopyright} 1997 Kluwer Academic Publishers.},
author = {Tammet, Tanel},
file = {:Users/jonaprieto/Desktop/Mendeley/1996 - Tammet - A Resolution Theorem Prover for Intuitionistic Logic.pdf:pdf},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {Automated theorem proving,Competition,Gandalf,Resolution,Subsumption},
number = {2},
pages = {199--204},
title = {{Gandalf}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0031108576{\&}partnerID=tZOtx3y1},
volume = {18},
year = {1997}
}
@incollection{Sicard-Ramirez2016,
address = {Medell{\'{i}}n, Colombia},
author = {Sicard-Ram{\'{i}}rez, Andr{\'{e}}s and Ospina-Giraldo, Juan-Fernando},
file = {:Users/jonaprieto/Desktop/Mendeley/2016 - Sicard-Ram{\'{i}}rez, Ospina-Giraldo - First-Order Proof Reconstruction (Research Proposal).pdf:pdf},
publisher = {Universidad EAFIT},
title = {{First-Order Proof Reconstruction (Research Proposal)}},
year = {2016}
}
@inproceedings{Burel,
abstract = {The $\lambda$$\Pi$-calculus modulo is a proof language that has been proposed as a proof standard for (re-)checking and interoperability. Resolution and superposition are proof-search methods that are used in state-of-the-art first-order automated theorem provers. We provide a shallow embedding of resolution and superposition proofs in the $\lambda$$\Pi$-calculus modulo, thus offering a way to check these proofs in a trusted setting, and to combine them with other proofs. We implement this embedding as a backend of the prover iProver Modulo.},
author = {Burel, Guillaume},
booktitle = {Third International Workshop on Proof Exchange for Theorem Proving,},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Burel - 2013 - A Shallow Embedding of Resolution and Superposition Proofs into the $\lambda$ $\Pi$ -Calculus.pdf:pdf},
keywords = {Interoperability,automatic theorem provers,proof checking,rewriting},
mendeley-tags = {Interoperability,automatic theorem provers,proof checking,rewriting},
pages = {1--15},
title = {{A Shallow Embedding of Resolution and Superposition Proofs into the $\lambda$ $\Pi$ -Calculus}},
year = {2013}
}
@book{quinlan2014c4,
author = {Quinlan, J Ross},
publisher = {Elsevier},
title = {{C4. 5: programs for machine learning}},
year = {2014}
}
@incollection{grzymala2006rough,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Foundations and novel approaches in data mining},
pages = {197--212},
publisher = {Springer},
title = {{Rough set strategies to data with missing attribute values}},
year = {2006}
}
@article{Bove2002,
abstract = {General recursive algorithms are such that the recursive calls are performed on arguments satisfying no condition that guarantees termination. Hence, there is no direct way of formalising them in type theory. The standard way of handling general recursion in type theory uses a well-founded recursion principle. Unfortunately, this way of formalising general recursive algorithms often produces unnecessarily long and complicated codes. On the other hand, functional programming languages like Haskell impose no restrictions on recursive programs, and then writing general recursive algorithms is straightforward. In addition, functional programs are usually short and self-explanatory. However, the existing frameworks for reasoning about the correctness of Haskell-like programs are weaker than the framework provided by type theory. The goal of this work is to present a method that combines the advantages of both programming styles when writing simple general recursive algorithms....},
author = {Bove, Ana},
doi = {10.1017/S0960129505004822},
file = {:Users/jonaprieto/Desktop/Mendeley/2002 - Bove - General recursion in type theory.pdf:pdf},
issn = {0346718X},
journal = {Doktorsavhandlingar vid Chalmers Tekniska Hogskola},
number = {1889},
pages = {39--58},
title = {{General recursion in type theory}},
year = {2002}
}
@misc{agdateam,
author = {Team, The Agda Developement},
edition = {Version 8.},
title = {{Agda 2.4.2.3}},
url = {http://wiki.portal.chalmers.se/agda/pmwiki.php},
year = {2015}
}
@misc{Prieto-Cubides2017,
abstract = {Collection of TPTP problems of propositional logic.},
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/ZENODO.817997},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Prieto-Cubides - 2017 - A Collection of Propositional Problems in TPTP Format.pdf:pdf;:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Prieto-Cubides - 2017 - A Collection of Propositional Problems in TPTP Format(2).pdf:pdf},
month = {jun},
title = {{A Collection of Propositional Problems in TPTP Format}},
url = {https://zenodo.org/record/817997{\#}.WYdjIbb5iAw},
year = {2017}
}
@incollection{greco2000dealing,
author = {Greco, SMBSR and Matarazzo, B and Slowinski, R},
booktitle = {Decision making: Recent developments and worldwide applications},
pages = {295--316},
publisher = {Springer},
title = {{Dealing with missing data in rough set analysis of multi-attribute and multi-criteria decision problems}},
year = {2000}
}
@incollection{grzymala2004data,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Transactions on Rough Sets I},
pages = {78--95},
publisher = {Springer},
title = {{Data with missing attribute values: Generalization of indiscernibility relation and rule induction}},
year = {2004}
}
@inproceedings{Bohme2011,
abstract = {Automatic provers that can produce proof certificates do not need to be trusted. The certificate can be checked by an independent tool, for example an LCF-style proof assistant such as IsabelleHOL or HOL. Currently, the design of proof formats is mostly dictated by internal constraints of automatic provers and less guided by applications such as checking of certificates. In the worst case, checking can be as involved as the actual proof search simply because important information is missing in the proof certificate. To address this and other issues, we describe design choices for proof formats that we consider both feasible for implementors of automatic provers as well as effective to simplify checking of certificates.},
author = {B{\"{o}}hme, Sascha and Weber, Tjark},
booktitle = {First International Workshop on Proof eXchange for Theorem Proving - PxTP 2011},
file = {:Users/jonaprieto/Desktop/Mendeley/2011 - B{\"{o}}hme, Weber - Designing Proof Formats A User's Perspective - Experience Report.pdf:pdf;:Users/jonaprieto/Desktop/Mendeley/2011 - B{\"{o}}hme, Weber - Designing Proof Formats A User's Perspective - Experience Report(2).pdf:pdf},
title = {{Designing Proof Formats: A User's Perspective - Experience Report}},
url = {http://hal.inria.fr/hal-00677244},
year = {2011}
}
@inproceedings{kerber1992chimerge,
author = {Kerber, Randy},
booktitle = {Proceedings of the tenth national conference on Artificial intelligence},
organization = {Aaai Press},
pages = {123--128},
title = {{Chimerge: Discretization of numeric attributes}},
year = {1992}
}
@inproceedings{Quinlan1989,
author = {Quinlan, John Ross},
booktitle = {Proc. of the Sixth Int. Workshop on Machine Learning},
pages = {164--168},
title = {{Unknown attribute values in induction}},
year = {1989}
}
@article{pawlak1982rough,
author = {Pawlak, Zdzis{\l}aw},
journal = {International Journal of Computer {\&} Information Sciences},
number = {5},
pages = {341--356},
publisher = {Springer},
title = {{Rough sets}},
volume = {11},
year = {1982}
}
@article{Mossakowski2005,
author = {Mossakowski, T. and Goguen, J. and Diaconescu, R. and Tarlecki, A.},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Mossakowski et al. - 2005 - What is a Logic.pdf:pdf},
journal = {Logica universalis},
keywords = {a version of abstract,abstract,abstract model,categorical logic,category theory,in computer science studies,institution theory,model theory that emerged,of software specifica-,the theory of institutions,theory,this paper builds on,universal logic},
pages = {113--133},
title = {{What is a Logic?}},
url = {http://cseweb.ucsd.edu/{~}goguen/pps/nel05.pdf},
year = {2005}
}
@book{Bertot2004,
abstract = {Coq is an interactive proof assistant for the development of mathematical theories and formally certified software. It is based on a theory called the calculus of inductive constructions, a variant of type theory. This book provides a pragmatic introduction to the development of proofs and certified programs using Coq. With its large collection of examples and exercises it is an invaluable tool for researchers, students, and engineers interested in formal methods and the development of zero-fault software.},
author = {Bertot, Yves and Cast{\'{e}}ran, Pierre},
booktitle = {Springer},
doi = {10.1007/978-3-662-07964-5},
file = {:Users/jonaprieto/Desktop/Mendeley/2004 - Bertot, Cast{\'{e}}ran - Interactive Theorem Proving and Program Development Coq'Art The Calculus of Inductive Constructions.pdf:pdf},
isbn = {3540208542},
title = {{Interactive Theorem Proving and Program Development: Coq'Art: The Calculus of Inductive Constructions}},
url = {http://books.google.nl/books/about/Interactive{\_}Theorem{\_}Proving{\_}and{\_}Program.html?id=m5w5PRj5Nj4C{\&}pgis=1},
year = {2004}
}
@book{Diestel2017,
abstract = {Almost two decades have passed since the appearance of those graph the- ory texts that still set the agenda for most introductory courses taught today. The canon created by those books has helped to identify some main fields of study and research, and will doubtless continue to influence the development of the discipline for some time to come.},
author = {Diestel, Reinhard},
doi = {10.1007/978-3-662-53622-3},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Diestel - Graph Theory.pdf:pdf},
isbn = {978-3-662-53621-6},
title = {{Graph Theory}},
url = {http://link.springer.com/10.1007/978-3-662-53622-3},
volume = {173},
year = {2017}
}
@article{garciarena2016investigation,
author = {{Garciarena Hualde}, Unai},
title = {{An investigation of imputation methods for discrete databases and multi-variate time series}},
year = {2016}
}
@phdthesis{Keller2013,
author = {Keller, Chantal},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Keller - 2013 - A Matter of Trust Skeptical Communication Between Coq and External Provers.pdf:pdf},
title = {{A Matter of Trust: Skeptical Communication Between Coq and External Provers}},
url = {https://hal.archives-ouvertes.fr/pastel-00838322/},
year = {2013}
}
@article{VanBenthem2010,
abstract = {We show how belief revision can be treated systematically in the format of dynamic- epistemic logic, when operators of conditional belief are added. The core engine consists of de{\{}{\}}nable update rules for changing plausibility relations between worlds, which have been proposed independently in the dynamic-epistemic literature on preference change. Our analysis yields two new types of modal result. First, we obtain complete logics for concrete mechanisms of belief revision, based on compositional reduction axioms. Next, we show how various ab- stract postulates for belief revision can be analyzed by standard modal frame correspondences for model-changing operations.},
author = {van Benthem, Johan},
file = {:Users/jonaprieto/Desktop/Mendeley/2010 - van Benthem - Modal Logic for Open Minds.pdf:pdf},
isbn = {0521527147},
journal = {Chart},
number = {199},
pages = {1--390},
pmid = {15957556},
title = {{Modal Logic for Open Minds}},
url = {http://fenrong.net/teaching/mljvb.pdf},
year = {2010}
}
@article{Zalta1995,
author = {Zalta, Edward N and Zalta, Edward N},
file = {:Users/jonaprieto/Desktop/Mendeley/1995 - Zalta, Zalta - Basic Concepts in Modal Logic.pdf:pdf},
journal = {Society},
pages = {1--91},
title = {{Basic Concepts in Modal Logic}},
year = {1995}
}
@incollection{grzymala2006rough2,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Rough Sets and Knowledge Technology},
pages = {58--67},
publisher = {Springer},
title = {{A rough set approach to data with missing attribute values}},
year = {2006}
}
@inproceedings{paulson2007source,
author = {Paulson, Lawrence C. and Susanto, Kong Woei},
booktitle = {TPHOLs},
file = {:Users/jonaprieto/Desktop/Mendeley/2007 - Paulson, Susanto - Source-level Proof Reconstruction For Interactive Theorem Proving.pdf:pdf},
organization = {Springer},
pages = {232--245},
title = {{Source-level Proof Reconstruction For Interactive Theorem Proving}},
volume = {4732},
year = {2007}
}
@article{Wadler1993,
abstract = {This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of `Propositions as Types'. The presentation of linear logic is simplified by basing it on the Logic of Unity.},
author = {Wadler, Philip},
doi = {10.1007/3-540-57182-5_12},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Wadler - 1993 - A taste of linear logic.pdf:pdf},
isbn = {3-540-57182-5},
journal = {Lecture Notes in Computer Science},
number = {September},
pages = {185--210},
title = {{A taste of linear logic}},
url = {http://www.springerlink.com/index/p555h611321h7240.pdf{\%}5Cnhttp://link.springer.com/10.1007/3-540-57182-5{\_}12},
volume = {711},
year = {1993}
}
@inproceedings{Bove2012,
abstract = {We propose a new approach to the computer-assisted verification of functional programs. We work in first order theories of functional programs which are obtained by extending Aczel's first order theory of combinatory formal arithmetic with positive inductive and coinductive predicates. Rather than building a special purpose system we implement our theories in Agda, a proof assistant for dependent type theory which can be used as a generic theorem prover. Agda provides support for interactive reasoning by encoding first order theories using the formulae-as-types principle. Further support is provided by off-the-shelf automatic theorem provers for first order logic which can be called by a program which translates Agda representations of first order formulae into the TPTP language understood by the provers. We show some examples where we combine interactive and automatic reasoning, covering both proof by induction and coinduction.},
author = {Bove, Ana and Dybjer, Peter and Sicard-Ram{\'{i}}rez, Andr{\'{e}}s},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-28729-9_7},
file = {:Users/jonaprieto/Desktop/Mendeley/2012 - Bove, Dybjer, Sicard-Ram{\'{i}}rez - Combining Interactive and Automatic Reasoning in First Order Theories of Functional Programs.pdf:pdf},
isbn = {9783642287282},
issn = {03029743},
pages = {104--118},
publisher = {Springer},
title = {{Combining Interactive and Automatic Reasoning in First Order Theories of Functional Programs}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-28729-9{\_}7},
volume = {7213 LNCS},
year = {2012}
}
@book{Mehta2005,
author = {Mehta, Dinesh P. and Sahni, Sartaj},
booktitle = {New York},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Mehta, Sahni - 2005 - Handbook of DATA STRUCTURES and APPLICATIONS.pdf:pdf},
isbn = {1584884355},
title = {{Handbook of DATA STRUCTURES and APPLICATIONS}},
year = {2005}
}
@article{Stump2013,
abstract = {Producing and checking proofs from {\{}SMT{\}} solvers is currently the most feasible method for achieving high confidence in the correctness of solver results. The diversity of solvers and relative complexity of {\{}SMT{\}} over, say, {\{}SAT{\}} means that flexibility, as well as performance, is a critical characteristic of a proof-checking solution for {\{}SMT{\}}. This paper describes such a solution, based on a Logical Framework with Side Conditions ({\{}LFSC{\}}). We describe the framework and show how it can be applied for flexible proof production and checking for two different {\{}SMT{\}} solvers, clsat and cvc3. We also report empirical results showing good performance relative to solver execution time.},
author = {Stump, Aaron and Oe, Duckki and Reynolds, Andrew and Hadarean, Liana and Tinelli, Cesare},
doi = {10.1007/s10703-012-0163-3},
file = {:Users/jonaprieto/Desktop/Mendeley/2013 - Stump et al. - SMT proof checking using a logical framework.pdf:pdf},
issn = {09259856},
journal = {Formal Methods in System Design},
keywords = {Edinburgh logical framework,LFSC,Proof checking,Satisfiability modulo theories},
number = {1},
pages = {91--118},
title = {{SMT proof checking using a logical framework}},
volume = {42},
year = {2013}
}
@article{Farber2016,
abstract = {Copyright ? by the paper's authors.Proof assistants based on higher-order logic frequently use first-order automated theorem provers as proof search mechanisms. The reconstruction of the proofs generated by common tools, such as MESON and Metis, typically involves the use of the axiom of choice to simulate the Skolemisation steps. In this paper we present a method to reconstruct the proofs without introducing Skolem functions. This enables us to integrate tactics that use first-order automated theorem provers in logics that feature neither the axiom of choice nor the definite description operator.},
author = {F{\"{a}}rber, Michael and Kaliszyk, Cezary},
file = {:Users/jonaprieto/Desktop/Mendeley/2016 - F{\"{a}}rber, Kaliszyk - No choice Reconstruction of first-order ATP proofs without skolem functions.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {Paar},
pages = {24--31},
title = {{No choice: Reconstruction of first-order ATP proofs without skolem functions}},
volume = {1635},
year = {2016}
}
@incollection{bouton2009,
abstract = {This article describes the first public version of the satisfiability modulo theory (SMT) solver veriT. It is open-source, proof-producing, and complete for quantifier-free formulas with uninterpreted functions and difference logic on real numbers and integers.},
address = {Berlin, Heidelberg},
author = {Bouton, Thomas and de Oliveira, Diego and D{\'{e}}harbe, David and Fontaine, Pascal},
booktitle = {Automated Deduction -- CADE-22: 22nd International Conference on Automated Deduction, Montreal, Canada, August 2-7, 2009. Proceedings},
doi = {10.1007/978-3-642-02959-2_12},
isbn = {978-3-642-02959-2},
pages = {151--156},
publisher = {Springer Berlin Heidelberg},
title = {{veriT: An Open, Trustable and Efficient SMT-Solver}},
year = {2009}
}
@article{Noschinski2015,
author = {Noschinski, Lars},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Noschinski - Formalizing Graph Theory and Planarity Certificates.pdf:pdf},
title = {{Formalizing Graph Theory and Planarity Certificates}},
year = {2015}
}
@article{kryszkiewicz1998rough,
author = {Kryszkiewicz, Marzena},
journal = {Information sciences},
number = {1},
pages = {39--49},
publisher = {Elsevier},
title = {{Rough set approach to incomplete information systems}},
volume = {112},
year = {1998}
}
@inproceedings{10.1007/3-540-60275-5_77,
abstract = {Among many fields of mathematics and computer science, discrete mathematics is one of the most difficult fields to formalize because we prove theorems using intuitive inferences that have not been rigorously formalized yet. This paper focuses on graph theory from discrete mathematics and formalizes planar graphs. Although planar graphs are usually defined by embeddings into the two-dimensional real space, this definition can hardly be used for actually developing a formal theory of planar graphs. In this paper, we take another approach; we inductively define planar graphs and prove their properties based on the inductive definition. Before the definition of planar graphs, the theory of cycles is also introduced and used as a foundation of planar graphs. As an application of the theory of planar graphs, Euler's formula is proved.},
address = {Berlin, Heidelberg},
author = {Yamamoto, Mitsuharu and Nishizaki, Shin-ya and Hagiya, Masami and Toda, Yozo},
booktitle = {Higher Order Logic Theorem Proving and Its Applications},
editor = {{Thomas Schubert}, E and Windley, Philip J and Alves-Foss, James},
file = {:Users/jonaprieto/Desktop/Mendeley/1995 - Yamamoto et al. - Formalization of planar graphs.pdf:pdf},
isbn = {978-3-540-44784-9},
pages = {369--384},
publisher = {Springer Berlin Heidelberg},
title = {{Formalization of planar graphs}},
year = {1995}
}
@incollection{Tammet1996,
author = {Tammet, Tanel},
doi = {10.1007/3-540-61511-3_65},
file = {:Users/jonaprieto/Desktop/Mendeley/1996 - Tammet - A Resolution Theorem Prover for Intuitionistic Logic.pdf:pdf},
pages = {2--16},
publisher = {Springer, Berlin, Heidelberg},
title = {{A Resolution Theorem Prover for Intuitionistic Logic}},
url = {http://link.springer.com/10.1007/3-540-61511-3{\_}65},
year = {1996}
}
@incollection{Pawlak2004,
author = {Pawlak, Zdzis{\l}aw},
booktitle = {Transactions on Rough Sets I},
pages = {1--58},
publisher = {Springer},
title = {{Some issues on rough sets}},
year = {2004}
}
@article{saar2007handling,
author = {Saar-Tsechansky, Maytal and Provost, Foster},
journal = {Journal of machine learning research},
number = {Jul},
pages = {1623--1657},
title = {{Handling missing values when applying classification models}},
volume = {8},
year = {2007}
}
@article{Schulz:AICOM-2002,
annote = {StS},
author = {Schulz, Stephan},
journal = {Journal of AI Communications},
number = {2/3},
pages = {111--126},
title = {{E -- A Brainiac Theorem Prover}},
volume = {15},
year = {2002}
}
@inproceedings{Lonsing2017,
abstract = {We present the latest major release version 6.0 of the quanti-fied Boolean formula (QBF) solver DepQBF, which is based on QCDCL. QCDCL is an extension of the conflict-driven clause learning (CDCL) paradigm implemented in state of the art propositional satisfiability (SAT) solvers. The Q-resolution calculus (QRES) is a QBF proof system which underlies QCDCL. QCDCL solvers can produce QRES proofs of QBFs in prenex conjunctive normal form (PCNF) as a byproduct of the solving process. In contrast to traditional QCDCL based on QRES, DepQBF 6.0 implements a variant of QCDCL which is based on a generalization of QRES. This generalization is due to a set of additional axioms and leaves the original Q-resolution rules unchanged. The generalization of QRES enables QCDCL to potentially produce exponentially shorter proofs than the traditional variant. We present an overview of the features imple-mented in DepQBF and report on experimental results which demonstrate the effectiveness of generalized QRES in QCDCL.},
address = {Gotenburg},
author = {Lonsing, Florian and Egly, Uwe},
booktitle = {International Conference on Automated Deduction},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Lonsing, Egly - DepQBF 6.0 A Search-Based QBF Solver Beyond Traditional QCDCL.pdf:pdf},
publisher = {Springer},
title = {{DepQBF 6.0: A Search-Based QBF Solver Beyond Traditional QCDCL}},
url = {https://arxiv.org/pdf/1702.08256.pdf},
year = {2017}
}
@article{Zhang2016,
abstract = {Interval-valued Information System (IvIS) is a generalized model of single-valued information system, in which the attribute values of objects are all interval values instead of single values. The attribute set in IvIS is not static but rather dynamically changing over time with the collection of new information, which results in the continuous updating of rough approximations for rough set-based data analysis. In this paper, on the basis of the similarity-based rough set model in IvIS, we develop incremental approaches for updating rough approximations in IvIS under attribute generalization, which refers to the dynamic changing of attributes. Firstly, increment relationships between the original rough approximations and the updated ones when adding or deleting an attribute set are analyzed, respectively. And the incremental mechanisms for updating rough approximations in IvIS are introduced, which carry out the computation using the previous results from the original data set along with new results. Then, the corresponding incremental algorithms are designed based on the proposed mechanisms. Finally, comparative experiments on data sets from UCI as well as artificial data sets are conducted, respectively. Experimental results show that the proposed incremental algorithms can effectively reduce the running time for the computation of rough approximations in comparison with the static algorithm.},
author = {Zhang, Yingying and Li, Tianrui and Luo, Chuan and Zhang, Junbo and Chen, Hongmei},
doi = {10.1016/j.ins.2016.09.018},
file = {:Users/jonaprieto/Desktop/Mendeley/2016 - Zhang et al. - Incremental updating of rough approximations in interval-valued information systems under attribute generalization.pdf:pdf},
isbn = {9783319257532},
issn = {00200255},
journal = {Information Sciences},
keywords = {Approximations,Incremental updating,Interval-valued information system,Rough set,Similarity degree},
pages = {1339--1351},
publisher = {Elsevier Inc.},
title = {{Incremental updating of rough approximations in interval-valued information systems under attribute generalization}},
url = {http://dx.doi.org/10.1016/j.ins.2016.09.018},
volume = {373},
year = {2016}
}
@incollection{sultana2015,
abstract = {Implementing proof reconstruction is difficult because it involves symbolic manipulations of formal objects whose representation varies between different systems. It requires significant knowledge of the source and target systems. One cannot simply re-target to another logic. We present a modular proof reconstruction system with separate components, specifying their behaviour and describing how they interact. This system is demonstrated and evaluated through an implementation to reconstruct proofs generated by Leo-II and Satallax in Isabelle HOL, and is shown to work better than the current method of rediscovering proofs using a select set of provers.},
address = {Cham},
author = {Sultana, Nik and Benzm{\"{u}}ller, Christoph and Paulson, Lawrence C.},
booktitle = {Frontiers of Combining Systems: 10th International Symposium, FroCoS 2015, Wroclaw, Poland, September 21-24, 2015, Proceedings},
doi = {10.1007/978-3-319-24246-0_16},
editor = {Lutz, Carsten and Ranise, Silvio},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Sultana, Benzm{\"{u}}ller, Paulson - Proofs and Reconstructions.pdf:pdf},
isbn = {978-3-319-24246-0},
pages = {256--271},
publisher = {Springer International Publishing},
title = {{Proofs and Reconstructions}},
url = {https://doi.org/10.1007/978-3-319-24246-0{\_}16},
year = {2015}
}
@article{Blanchette2016,
author = {Blanchette, Jasmin and B{\"{o}}hme, Sascha and Fleury, Mathias and Smolka, Steffen Juilf and Steckermeier, Albert},
doi = {10.1007/s10817-015-9335-3},
file = {:Users/jonaprieto/Desktop/Mendeley/2016 - Blanchette et al. - Semi-intelligible Isar Proofs from Machine-Generated Proofs.pdf:pdf},
issn = {15730670},
journal = {Journal of Automated Reasoning},
keywords = {Automatic theorem provers,Natural deduction,Proof assistants},
number = {2},
pages = {155--200},
publisher = {Journal of Automated Reasoning},
title = {{Semi-intelligible Isar Proofs from Machine-Generated Proofs}},
url = {http://dx.doi.org/10.1007/s10817-015-9335-3},
volume = {56},
year = {2016}
}
@article{gabrys2002neuro,
author = {Gabrys, Bogdan},
journal = {International Journal of Approximate Reasoning},
number = {3},
pages = {149--179},
publisher = {Elsevier},
title = {{Neuro-fuzzy approach to processing inputs with missing values in pattern recognition problems}},
volume = {30},
year = {2002}
}
@article{wong1987synthesizing,
author = {Wong, Andrew K C and Chiu, David K Y},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {6},
pages = {796--805},
publisher = {IEEE},
title = {{Synthesizing statistical knowledge from incomplete mixed-mode data}},
year = {1987}
}
@article{Gallian2000,
author = {Gallian, Joseph A and Higgins, Aparna W},
file = {:Users/jonaprieto/Desktop/Mendeley/2000 - Gallian, Higgins - Helping Students Present Their Research.pdf:pdf},
pages = {289--295},
title = {{Helping Students Present Their Research}},
year = {2000}
}
@incollection{armand2011,
abstract = {We present a way to enjoy the power of SAT and SMT provers in Coq without compromising soundness. This requires these provers to return not only a yes/no answer, but also a proof witness that can be independently rechecked. We present such a checker, written and fully certified in Coq. It is conceived in a modular way, in order to tame the proofs' complexity and to be extendable. It can currently check witnesses from the SAT solver ZChaff and from the SMT solver veriT. Experiments highlight the efficiency of this checker. On top of it, new reflexive Coq tactics have been built that can decide a subset of Coq's logic by calling external provers and carefully checking their answers.},
address = {Berlin, Heidelberg},
author = {Armand, Michael and Faure, Germain and Gr{\'{e}}goire, Benjamin and Keller, Chantal and Th{\'{e}}ry, Laurent and Werner, Benjamin},
booktitle = {Certified Programs and Proofs: First International Conference, CPP 2011, Kenting, Taiwan, December 7-9, 2011. Proceedings},
doi = {10.1007/978-3-642-25379-9_12},
isbn = {978-3-642-25379-9},
pages = {135--150},
publisher = {Springer Berlin Heidelberg},
title = {{A Modular Integration of SAT/SMT Solvers to Coq through Proof Witnesses}},
url = {https://doi.org/10.1007/978-3-642-25379-9{\_}12},
year = {2011}
}
@article{Zhang2015,
abstract = {As the volume of data grows at an unprecedented rate, large-scale data mining and knowledge discovery present a tremendous challenge. Rough set theory, which has been used successfully in solving problems in pattern recognition, machine learning, and data mining, centers around the idea that a set of distinct objects may be approximated via a lower and upper bound. In order to obtain the benefits that rough sets can provide for data mining and related tasks, efficient computation of these approximations is vital. The recently introduced cloud computing model, MapReduce, has gained a lot of attention from the scientific community for its applicability to large-scale data analysis. In previous research, we proposed a MapReduce-based method for computing approximations in parallel, which can efficiently process complete data but fails in the case of missing (incomplete) data. To address this shortcoming, three different parallel matrix-based methods are introduced to process large-scale, incomplete data. All of them are built on MapReduce and implemented on Twister that is a lightweight MapReduce runtime system. The proposed parallel methods are then experimentally shown to be efficient for processing large-scale data.},
author = {Zhang, Junbo and Wong, Jian Syuan and Pan, Yi and Li, Tianrui},
doi = {10.1109/TKDE.2014.2330821},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Zhang et al. - A parallel matrix-based method for computing approximations in incomplete information systems.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {MapReduce,Rough sets,data mining,incomplete information systems,matrix},
number = {2},
pages = {326--339},
title = {{A parallel matrix-based method for computing approximations in incomplete information systems}},
volume = {27},
year = {2015}
}
@article{stefanowski2001incomplete,
author = {Stefanowski, Jerzy and Tsoukias, Alexis},
journal = {Computational Intelligence},
number = {3},
pages = {545--566},
publisher = {Wiley Online Library},
title = {{Incomplete information tables and rough classification}},
volume = {17},
year = {2001}
}
@phdthesis{Sicard2015,
abstract = {We propose a new approach to computer-assisted verification of lazy functional programs where functions can be defined by general recursion. We work in first-order theories of functional programs which are obtained by translating Dybjer's programming logic (Dybjer, P. [1985]. Program Veri- fication in a Logical Theory of Constructions. In: Functional Programming Languages and Computer Architecture. Ed. by Jouannaud, J.-P. Vol. 201. Lecture Notes in Computer Science. Springer, pp. 334–349) into a first-order theory, and by extending this programming logic with new (co-)inductive predicates. Rather than building a special purpose system, we formalise our theories in Agda, a proof assistant for dependent type theory which can be used as a generic theorem prover. Agda provides support for interact- ive reasoning by representing first-order theories using the propositions-as- types principle. Further support is provided by off-the-shelf automatic the- orem provers for first-order-logic called by a Haskell program that translates our Agda representations of first-order formulae into the TPTP language understood by the provers. We show some examples where we combine interactive and automatic reasoning, covering both proofs by induction and co- induction. The examples include functions defined by structural recursion, simple general recursion, nested recursion, higher-order recursion, guarded and unguarded co-recursion. Keywords:},
author = {Sicard-Ram{\'{i}}rez, Andr{\'{e}}s and Bove, Ana and Dybjer, Peter},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Sicard-Ram{\'{i}}rez, Bove, Dybjer - Reasoning about Functional Programs by Combining Interactive and Automatic Proofs.pdf:pdf},
keywords = {automatic proofs,correctness,first-order theories,functional program,general recursion,interactive proofs,lazy evaluation,total lan- guages,type theory},
school = {Universidad de la Rep{\{}{\'{u}}{\}}blica},
title = {{Reasoning about Functional Programs by Combining Interactive and Automatic Proofs}},
url = {https://www.colibri.udelar.edu.uy/handle/123456789/4715},
year = {2015}
}
@article{hillenbrand1997,
abstract = {Waldmeister is a high-performance theorem prover for unit equational first-order logic. In the making of Waldmeister, we have applied an engineering approach, identifying the critical points with respect to efficiency in time and space. Our logical three-level system model consists of the basic operations on the lowest level, where we put great stress on efficient data structures and algorithms. For the middle level, where the inference steps are aggregated into an inference machine, flexible adjustment has proven essential during experimental evaluation. The top level holds control strategy and reduction ordering. Although at this level only standard strategies are employed, really large proof tasks have been managed in reasonable time.},
author = {Hillenbrand, Thomas and Buch, Arnim and Vogt, Roland and L{\"{o}}chner, Bernd},
doi = {10.1023/A:1005872405899},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
number = {2},
pages = {265--270},
title = {{WALDMEISTER - High-Performance Equational Deduction}},
url = {https://doi.org/10.1023/A:1005872405899},
volume = {18},
year = {1997}
}
@inproceedings{lakshminarayan1996imputation,
author = {Lakshminarayan, Kamakshi and Harp, Steven A and Goldman, Robert P and Samad, Tariq and Others},
booktitle = {KDD},
pages = {140--145},
title = {{Imputation of Missing Data Using Machine Learning Techniques.}},
year = {1996}
}
@book{Baier2008,
abstract = {Our growing dependence on increasingly complex computer and software systems necessitates the development of formalisms, techniques, and tools for assessing functional properties of these systems. One such technique that has emerged in the last twenty years is model checking, which systematically (and automatically) checks whether a model of a given system satisfies a desired property such as deadlock freedom, invariants, or request-response properties. This automated technique for verification and debugging has developed into a mature and widely used approach with many applications. Principles of Model Checking offers a comprehensive introduction to model checking that is not only a text suitable for classroom use but also a valuable reference for researchers and practitioners in the field. The book begins with the basic principles for modeling concurrent and communicating systems, introduces different classes of properties (including safety and liveness), presents the notion of fairness, and provides automata- based algorithms for these properties. It introduces the temporal logics LTL and CTL, compares them, and covers algorithms for verifying these logics, discussing real-time systems as well as systems subject to random phenomena. Separate chapters treat such efficiency-improving techniques as abstraction and symbolic manipulation. The book includes an extensive set of examples (most of which run through several chapters) and a complete set of basic results accompanied by detailed proofs. Each chapter concludes with a summary, bibliographic notes, and an extensive list of exercises of both practical and theoretical nature.},
author = {Baier, Christel and Katoen, Joost-Pieter},
booktitle = {MIT Press},
doi = {10.1093/comjnl/bxp025},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Baier, Katoen - 2008 - Principles Of Model Checking.pdf:pdf;:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Baier, Katoen - 2008 - Principles Of Model Checking(2).pdf:pdf},
isbn = {9780262026499},
issn = {00155713},
pages = {I--XVII, 1--975},
pmid = {11275744},
title = {{Principles Of Model Checking}},
url = {http://mitpress.mit.edu/books/principles-model-checking},
volume = {950},
year = {2008}
}
@inproceedings{paulson2010three,
author = {Paulson, Lawrence C. and Blanchette, Jasmin},
booktitle = {PAAR@ IJCAR},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Paulson, Blanchette - 2010 - Three Years Of Experience with Sledgehammer, A Practical Link Between Automatic And Interactive Theorem Pro.pdf:pdf},
pages = {1--10},
title = {{Three Years Of Experience with Sledgehammer, A Practical Link Between Automatic And Interactive Theorem Provers.}},
year = {2010}
}
@phdthesis{Bergeron1998,
abstract = {The combinatorial theory of species, introduced by Joyal in 1980, provides a unified understanding of the use of generating functions for both labeled and unlabeled structures as well as a tool for the specification and analysis of these structures. This key reference presents the basic elements of the theory and gives a unified account of its developments and applications. The authors offer a modern introduction to the use of various generating functions, with applications to graphical enumeration, Polya Theory and analysis of data structures in computer science, and to other areas such as special functions, functional equations, asymptotic analysis, and differential equations.},
author = {Yorgey, Brent Abraham},
file = {:Users/jonaprieto/Desktop/Mendeley/2014 - Yorgey - Combinatorial Species And Labelled Structures.pdf:pdf},
isbn = {0521573238},
pages = {206},
title = {{Combinatorial Species And Labelled Structures}},
url = {https://books.google.com/books?id=83odtWY4eogC{\&}pgis=1},
year = {2014}
}
@techreport{Gomez-Londono2015,
author = {G{\'{o}}mez-Londo{\~{n}}o, Alejandro},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - G{\'{o}}mez-Londo{\~{n}}o - Proof Reconstruction Parsing Proofs.pdf:pdf},
institution = {EAFIT University},
title = {{Proof Reconstruction: Parsing Proofs}},
url = {http://repository.eafit.edu.co/handle/10784/5484},
year = {2015}
}
@misc{Coquand1992,
abstract = {Pattern Matching with Dependent Types Thierry Coquand Chalmers University Preliminary version, June 1992 Introduction This note deals with notation in type theory. The de nition of a function by pattern matching is by now common, and quite important in practice, in ...},
author = {Coquand, T.},
booktitle = {Informal proceedings of Logical Frameworks},
doi = {10.1.1.37.9541},
file = {:Users/jonaprieto/Desktop/Mendeley/1992 - Coquand - Pattern matching with dependent types.pdf:pdf},
number = {June},
pages = {1--14},
title = {{Pattern matching with dependent types}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.9541{\&}rep=rep1{\&}type=pdf{\#}page=69},
volume = {1992},
year = {1992}
}
@misc{Jordan2003,
abstract = {This book is designed for students embarking on further studies through the medium of English. This course is suitable for students at Cambrdige First Certificate level and above. The third edition has been revised to take account of feedback from users and recent developments in the teaching of writing.},
author = {Jordan, R R},
file = {:Users/jonaprieto/Desktop/Mendeley/2003 - Jordan - Academic writing course study skills in English.pdf:pdf},
isbn = {0582400198},
keywords = {engels,english,grammar,grammatica,languages,schrijfvaardigheden,studieboeken,talen,textbooks,writing skills},
pmid = {10682171},
title = {{Academic writing course : study skills in English}},
year = {2003}
}
@book{Hall/CRC2016,
abstract = {The fusion between graph theory and combinatorial optimization has led to theoretically profound and practically useful algorithms, yet there is no book that currently covers both areas together. Handbook of Graph Theory, Combinatorial Optimization, and Algorithms is the first to present a unified, comprehensive treatment of both graph theory and combinatorial optimization. Divided into 11 cohesive sections, the handbook's 44 chapters focus on graph theory, combinatorial optimization, and algorithmic issues. The book provides readers with the algorithmic and theoretical foundations to: Understand phenomena as shaped by their graph structures Develop needed algorithmic and optimization tools for the study of graph structures Design and plan graph structures that lead to certain desirable behavior With contributions from more than 40 worldwide experts, this handbook equips readers with the necessary techniques and tools to solve problems in a variety of applications. Readers gain exposure to the theoretical and algorithmic foundations of a wide range of topics in graph theory and combinatorial optimization, enabling them to identify (and hence solve) problems encountered in diverse disciplines, such as electrical, communication, computer, social, transportation, biological, and other networks.},
author = {Hall/CRC and Chapman},
file = {:Users/jonaprieto/Desktop/Mendeley/2016 - HallCRC, Chapman - Handbook of Graph Theory , Combinatorial Optimization , and Algorithms Series Editor Sartaj Sahni.pdf:pdf},
isbn = {9781420011074},
pages = {1226},
title = {{Handbook of Graph Theory , Combinatorial Optimization , and Algorithms Series Editor : Sartaj Sahni}},
year = {2016}
}
@book{humberstone2011,
author = {Humberstone, L.},
publisher = {MIT Press},
title = {{The Connectives}},
year = {2011}
}
@inproceedings{grzymala1997modified,
author = {Grzymala-Busse, Jerzy W and Wang, Arthur Y},
booktitle = {Proc. of the Fifth International Workshop on Rough Sets and Soft Computing (RSSC'97) at the Third Joint Conference on Information Sciences (JCIS'97), Research Triangle Park, NC},
pages = {69--72},
title = {{Modified algorithms LEM1 and LEM2 for rule induction from data with missing attribute values}},
year = {1997}
}
@article{chiu1986synthesizing,
author = {Chiu, David K Y. and Wong, Andrew K. C.},
journal = {Systems, Man and Cybernetics, IEEE Transactions on},
number = {2},
pages = {251--259},
publisher = {IEEE},
title = {{Synthesizing knowledge: A cluster analysis approach using event covering}},
volume = {16},
year = {1986}
}
@inproceedings{Klein2009,
address = {New York, New York, USA},
author = {Klein, Gerwin and Norrish, Michael and Sewell, Thomas and Tuch, Harvey and Winwood, Simon and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal},
booktitle = {Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles - SOSP '09},
doi = {10.1145/1629575.1629596},
isbn = {9781605587523},
keywords = {isabelle/hol,l4,microkernel,sel4},
pages = {207},
publisher = {ACM Press},
title = {{seL4}},
url = {http://portal.acm.org/citation.cfm?doid=1629575.1629596},
year = {2009}
}
@phdthesis{kanso:MResThesis:2013,
abstract = {This project studied whether a digital interlocking which had been pro- grammed with ladder logic (Boolean program) would obey generic safety properties. This was carried out by translating the ladder logic into an alternate representation and applying various techniques to allow specifica- tion of safety properties. Finally, a proof engine was used to formally verify if these properties were fulfilled and if they are not, then human readable documentation would be generated. III},
annote = {{\{}M{\}}aster of {\{}R{\}}esearch thesis, Dept.{\~{}}of Computer Science, Swansea University, Swansea SA2 8PP, UK.
Available from http://www.swan.ac.uk/{\$}{\~{}}{\$}csetzer/articlesFromOthers/index.html},
author = {Kanso, Karim},
file = {:Users/jonaprieto/Desktop/Mendeley/2010 - Kanso - Formal Verification of Ladder Logic.pdf:pdf},
school = {Swansea University},
title = {{Formal Verification of Ladder Logic}},
url = {http://www.cs.swan.ac.uk/{~}csetzer/articlesFromOthers/kanso/karimKansoMResThesisFormalVerificationOfLadderLogic.pdf},
year = {2010}
}
@book{Dasgupta,
author = {Dasgupta, Sanjoy and Papadimitriou, Christos and Vazirani, Umesh},
file = {:Users/jonaprieto/.Trash/Unknown - Dasgupta, Papadimitriou, Vazirani - Sanjoy Dasgupta Christos Papadimitriou Umesh Vazirani.pdf:pdf},
isbn = {9780073523408},
title = {{Sanjoy Dasgupta Christos Papadimitriou Umesh Vazirani}}
}
@article{sutcliffe2004tstp,
author = {Sutcliffe, Geoff and Zimmer, J{\"{u}}rgen and Schulz, Stephan},
journal = {Distributed Constraint Problem Solving and Reasoning in Multi-Agent Systems},
pages = {201--215},
title = {{TSTP data-exchange formats for automated theorem proving tools}},
volume = {112},
year = {2004}
}
@inproceedings{Farber2015,
abstract = {Metis is an automated theorem prover based on ordered paramodulation. It is widely employed in the interactive theorem provers Isabelle/HOL and HOL4 to automate proofs as well as reconstruct proofs found by automated provers. For both these purposes, the tableaux-based MESON tactic is frequently used in HOL Light. However, paramodulation-based provers such as Metis perform better on many problems involving equality. We created a Metis-based tactic in HOL Light which translates HOL problems to Metis, runs an OCaml version of Metis, and reconstructs proofs in Metis' paramodulation calculus as HOL proofs. We evaluate the performance of Metis as proof reconstruction method in HOL Light. 1},
author = {F{\"{a}}rber, Michael and Kaliszyk, Cezary},
booktitle = {GCAI 2015. Global Conference on Artificial Intelligence Metis-based},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - F{\"{a}}rber, Kaliszyk - Metis-based Paramodulation Tactic for HOL Light.pdf:pdf},
pages = {127--136},
title = {{Metis-based Paramodulation Tactic for HOL Light}},
volume = {36},
year = {2015}
}
@article{junninen2004methods,
author = {Junninen, Heikki and Niska, Harri and Tuppurainen, Kari and Ruuskanen, Juhani and Kolehmainen, Mikko},
journal = {Atmospheric Environment},
number = {18},
pages = {2895--2907},
publisher = {Elsevier},
title = {{Methods for imputation of missing values in air quality data sets}},
volume = {38},
year = {2004}
}
@book{TobiasNipkow2016,
author = {Nipkow, Tobias and Klein, Gerwin},
doi = {10.1007/978-3-319-10542-0},
file = {:Users/jonaprieto/Desktop/Mendeley/2016 - Nipkow, Klein - Concrete Semantics.pdf:pdf},
isbn = {9783319105413},
pages = {1--2},
title = {{Concrete Semantics}},
year = {2016}
}
@misc{denivelle2003,
author = {{De Nivelle}, Hans},
title = {{Bliksem 1.10 User Manual}},
url = {http://www.ii.uni.wroc.pl/{~}nivelle/software/bliksem.}
}
@misc{norrish2017hol,
author = {Norrish, Michael and Slind, Konrad},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Norrish, Slind - The HOL system description.pdf:pdf},
title = {{The HOL system description}},
url = {https://sourceforge.net/projects/hol/files/hol/kananaskis-11/kananaskis-11-reference.pdf/download},
year = {2017}
}
@article{Beringer2015,
abstract = {We have proved, with machine-checked proofs in Coq, that an OpenSSL implementation of HMAC with SHA-256 correctly implements its FIPS functional specifi-cation and that its functional specification guarantees the expected cryptographic properties. This is the first machine-checked cryptographic proof that combines a source-program implementation proof, a compiler-correctness proof, and a cryptographic-security proof, with no gaps at the specification interfaces. The verification was done using three systems within the Coq proof assistant: the Foundational Cryptogra-phy Framework, to verify crypto properties of functional specs; the Verified Software Toolchain, to verify C pro-grams w.r.t. functional specs; and CompCert, for verified compilation of C to assembly language.},
author = {Beringer, Lennart and Petcher, Adam and Ye, Katherine Q and Appel, Andrew W},
doi = {10.1145/3133956.3133974},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Beringer et al. - Verified correctness and security of OpenSSL HMAC.pdf:pdf},
isbn = {9781931971232},
journal = {Usenix Sec},
title = {{Verified correctness and security of OpenSSL HMAC}},
year = {2015}
}
@inproceedings{grzymala2004rough,
author = {Grzymala-Busse, Jerzy W and Siddhaye, Sachin},
booktitle = {Proceedings of the IPMU},
pages = {923--930},
title = {{Rough set approaches to rule induction from incomplete data}},
volume = {2},
year = {2004}
}
@article{Weber2009,
abstract = {This paper describes the integration of zChaff and MiniSat, currently two leading SAT solvers, with Higher Order Logic (HOL) theorem provers. Both SAT solvers generate resolution-style proofs for (instances of) propositional tautologies. These proofs are verified by the theorem provers. The presented approach significantly improves the provers' performance on propositional problems, and exhibits counterexamples for unprovable conjectures. It is also shown that LCF-style theorem provers can serve as viable proof checkers even for large SAT problems. An efficient representation of the propositional problem in the theorem prover turns out to be crucial; several possible solutions are discussed. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Weber, Tjark and Amjad, Hasan},
doi = {10.1016/j.jal.2007.07.003},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Weber, Amjad - 2009 - Efficiently checking propositional refutations in HOL theorem provers.pdf:pdf},
issn = {15708683},
journal = {Journal of Applied Logic},
keywords = {Interactive theorem proving,LCF-style proof checking,Propositional resolution},
number = {1},
pages = {26--40},
title = {{Efficiently checking propositional refutations in HOL theorem provers}},
volume = {7},
year = {2009}
}
@misc{keeler1998method,
annote = {US Patent 5,842,189},
author = {Keeler, James David and Hartman, Eric Jon and Ferguson, Ralph Bruce},
publisher = {Google Patents},
title = {{Method for operating a neural network with missing and/or incomplete data}},
year = {1998}
}
@article{Paulson1986,
abstract = {Martin-L{\"{o}}f's Intuitionistic Theory of Types is becoming popular for formal reasoning about computer programs. To handle recursion schemes other than primitive recursion, a theory of well-founded relations is presented. Using primitive recursion over higher types, induction and recursion are formally derived for a large class of well-founded relations. Included are {\textless} on natural numbers, and relations formed by inverse images, addition, multiplication, and exponentiation of other relations. The constructions are given in full detail to allow their use in theorem provers for Type Theory, such as PRL. The theory is compared with work in the field of ordinal recursion over higher types. {\textcopyright} 1986, Academic Press Inc. (London) Ltd.. All rights reserved.},
author = {Paulson, Lawrence C.},
doi = {10.1016/S0747-7171(86)80002-5},
file = {:Users/jonaprieto/Desktop/Mendeley/1986 - Paulson - Constructing recursion operators in intuitionistic type theory.pdf:pdf},
issn = {07477171},
journal = {Journal of Symbolic Computation},
number = {4},
pages = {325--355},
title = {{Constructing recursion operators in intuitionistic type theory}},
volume = {2},
year = {1986}
}
@phdthesis{Prieto-Cubides2017a,
abstract = {We describe a syntactical proof-reconstruction approach to verify proofs gener- ated by Metis prover to problems in classical propositional logic. To verify these Metis proofs, we developed a tool able to translate such derivations to Agda proof-terms. We formalise in type theory each inference rule found in Metis derivations. This allowed us to type-check with Agda, Metis proofs step-by-step. We leave it for future work, the opportunity to extend this work to support other provers, proof-assistants, and formalisms.},
author = {Prieto-Cubides, Jonathan},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Prieto-Cubides - Reconstructing Propositional Proofs in Type Theory.pdf:pdf},
keywords = {agda,automatic theorem prover,proof-assistant,proof-reconstruction,theorem-proving,type theory},
mendeley-tags = {agda,proof-reconstruction,theorem-proving},
school = {Universidad EAFIT},
title = {{Reconstructing Propositional Proofs in Type Theory}},
year = {2017}
}
@article{jerez2010missing,
author = {Jerez, Jos{\'{e}} M. and Molina, Ignacio and Garc{\'{i}}a-Laencina, Pedro J. and Alba, Emilio and Ribelles, Nuria and Martin, Miguel and Franco, Leonardo},
journal = {Artificial intelligence in medicine},
number = {2},
pages = {105--115},
publisher = {Elsevier},
title = {{Missing data imputation using statistical and machine learning methods in a real breast cancer problem}},
volume = {50},
year = {2010}
}
@inproceedings{grzymala2000comparison,
author = {Grzymala-Busse, Jerzy W and Hu, Ming},
booktitle = {Rough sets and current trends in computing},
organization = {Springer},
pages = {378--385},
title = {{A comparison of several approaches to missing attribute values in data mining}},
year = {2000}
}
@inproceedings{Weidenbach2009,
abstract = {SPASS is an automated theorem prover for full first-order logic with equality and a number of non-classical logics. This system description provides an overview of our recent developments in SPASS 3.5 including subterm contextual rewriting, improved split backtracking, a significantly faster FLOTTER implementation with additional control flags, completely symmetric implementation of forward and backward redundancy criteria, faster parsing with improved support for big files, faster and extended sort module, and support for include commands in input files. Finally, SPASS 3.5 can now parse files in TPTP syntax, comes with a new converter tptp2dfg and is distributed under a BSD style license.},
author = {Weidenbach, Christoph and Dimova, Dilyana and Fietzke, Arnaud and Kumar, Rohit and Suda, Martin and Wischnewski, Patrick},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02959-2_10},
isbn = {3642029582},
issn = {03029743},
pages = {140--145},
title = {{SPASS version 3.5}},
volume = {5663 LNAI},
year = {2009}
}
@misc{AgdaProp,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.398852},
title = {{A Library for Classical Propositional Logic in Agda}},
url = {https://doi.org/10.5281/zenodo.398852},
year = {2017}
}
@misc{AgdaMetis,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.398862},
title = {{Metis Prover Reasoning for Propositional Logic in Agda}},
url = {https://doi.org/10.5281/zenodo.398862},
year = {2017}
}
@book{Steinberg2008,
abstract = {Perfect Phrases to stand out on the TOEFL-for the more than 800,000 people who take the test To be accepted into most North American undergraduate and graduate programs, international students must take and pass the Test of English as a Foreign Language. Perfect Phrases for the TOEFL Speaking and Writing Sections gives you all the phrases and most commonly used words you need to excel on both the writing and speaking sections of the test. Presented in the easy-to-understand Perfect Phrases format, these phrases allow you to effectively communicate and express yourself in standard American English, and to score your very best on the test.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Steinberg, Roberta},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/jonaprieto/Desktop/Mendeley/2008 - Steinberg - Perfect Phrases for the TOEFL Speaking and Writing Sections.pdf:pdf},
isbn = {0071592474},
issn = {0717-6163},
pages = {224},
pmid = {15003161},
title = {{Perfect Phrases for the TOEFL Speaking and Writing Sections}},
url = {https://books.google.com/books?id=S8woospF2SYC{\&}pgis=1},
year = {2008}
}
@article{hong2002learning,
author = {Hong, Tzung-Pei and Tseng, Li-Huei and Wang, Shyue-Liang},
journal = {Expert Systems with Applications},
number = {4},
pages = {285--293},
publisher = {Elsevier},
title = {{Learning rules from incomplete training examples by rough sets}},
volume = {22},
year = {2002}
}
@incollection{Book,
author = {Book, The},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Book - Unknown - TLDR Pages.pdf:pdf},
title = {{TLDR Pages}}
}
@inproceedings{Brown2012,
abstract = {Satallax is an automatic higher-order theorem prover that generates propositional clauses encoding (ground) tableau rules and uses MiniSat to test for unsatisfiability. We describe the implementation, focusing on flags that control search and examples that illustrate how the search proceeds.},
author = {Brown, Chad E.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-31365-3_11},
isbn = {9783642313646},
issn = {03029743},
keywords = {higher-order logic,higher-order theorem proving,simple type theory},
pages = {111--117},
title = {{Satallax: An automatic higher-order prover}},
volume = {7364 LNAI},
year = {2012}
}
@incollection{grzymala2008three,
author = {Grzymala-Busse, Jerzy W},
booktitle = {Data Mining: Foundations and Practice},
pages = {139--152},
publisher = {Springer},
title = {{Three approaches to missing attribute values: A rough set perspective}},
year = {2008}
}
@article{kim1977treatment,
author = {Kim, Jae-On and Curry, James},
journal = {Sociological Methods {\&} Research},
number = {2},
pages = {215--240},
publisher = {Sage Publications},
title = {{The treatment of missing data in multivariate analysis}},
volume = {6},
year = {1977}
}
@book{Rahman2017,
author = {Rahman, Md. Saidur},
doi = {10.1007/978-3-319-49475-3},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Rahman - Basic Graph Theory.pdf:pdf},
isbn = {978-3-319-49474-6},
issn = {15715078},
pmid = {1000172844},
title = {{Basic Graph Theory}},
url = {http://link.springer.com/10.1007/978-3-319-49475-3},
year = {2017}
}
@book{Chlipala2017,
author = {Chlipala, Adam},
booktitle = {MIT Press},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Chlipala - Certified Programming with Dependent Types.pdf:pdf},
isbn = {9780262317863},
title = {{Certified Programming with Dependent Types}},
url = {http://adam.chlipala.net/cpdt/cpdt.pdf},
year = {2017}
}
@inproceedings{Moskewicz2001,
abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search - especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.},
author = {Moskewicz, Matthew W and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
booktitle = {Proceedings of the 38th Design Automation Conference (DAC 2001)},
doi = {http://doi.acm.org/10.1145/378239.379017},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Moskewicz et al. - 2001 - Chaff engineering an efficient SAT solver.pdf:pdf},
isbn = {1-58113-297-2},
issn = {0738-100X},
keywords = {Branching Heuristics,CDCL,Chaff,Lazy Data-Structure,SAT,Solver,VSIDS,Watched literals},
pages = {530--535},
title = {{Chaff: engineering an efficient SAT solver}},
year = {2001}
}
@book{VanBenthem1990a,
abstract = {This volume presents a panorama of the applications of logical tools and methods in the formal analysis of natural language. since a number of few developments in philosophical logic were originally stimulated by concern arising in the semantic analysis of natural language discourse, the chapters in this volume provide some criteria of evaluation of the applications of work in philosophical logic. in revealing both the adequacies and inadequacies of logical investigations in the semantic structures of natural discourse, these chapters also point the way to future developments in philosophical logic in general and thus close again the circle of inquiry relating logic and language.},
author = {van Benthem, Johan},
booktitle = {Language},
doi = {10.2307/414900},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/van Benthem - 1990 - Handbook of Philosophical Logic, Vol. IV Topics in the Philosophy of Language.pdf:pdf},
isbn = {0198537816},
issn = {00326585},
number = {2},
pages = {555},
title = {{Handbook of Philosophical Logic, Vol. IV: Topics in the Philosophy of Language}},
volume = {66},
year = {1990}
}
@book{pawlak2012rough,
author = {Pawlak, Zdzis{\l}aw},
publisher = {Springer Science {\&} Business Media},
title = {{Rough sets: Theoretical aspects of reasoning about data}},
volume = {9},
year = {2012}
}
@book{little2014statistical,
author = {Little, Roderick J A and Rubin, Donald B},
publisher = {John Wiley {\&} Sons},
title = {{Statistical analysis with missing data}},
year = {2014}
}
@article{sutcliffe2009,
abstract = {This paper describes the First-Order Form (FOF) and Clause Normal Form (CNF) parts of the TPTP problem library, and the associated infrastructure. TPTP v3.5.0 was the last release containing only FOF and CNF problems, and thus serves as the exemplar. This paper summarizes the history and development of the TPTP, describes the structure and contents of the TPTP, and gives an overview of TPTP related projects and tools.},
author = {Sutcliffe, Geoff},
doi = {10.1007/s10817-009-9143-8},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
month = {jul},
number = {4},
pages = {337},
title = {{The TPTP Problem Library and Associated Infrastructure}},
url = {https://doi.org/10.1007/s10817-009-9143-8},
volume = {43},
year = {2009}
}
@article{meng2006automation,
author = {Meng, Jia and Quigley, Claire and Paulson, Lawrence C.},
journal = {Information and computation},
number = {10},
pages = {1575--1596},
publisher = {Elsevier},
title = {{Automation for Interactive Proof: First Prototype}},
volume = {204},
year = {2006}
}
@article{Turaev,
author = {Turaev, Vladimir and Virelizier, Alexis},
doi = {10.1007/978-3-319-49834-8},
file = {:Users/jonaprieto/Desktop/Mendeley/Unknown - Turaev, Virelizier - Monoidal Categories and Topological Field Theory.pdf:pdf},
isbn = {9783319498331},
title = {{Monoidal Categories and Topological Field Theory}}
}
@phdthesis{Fleury2015,
abstract = {Various methods have been developed for solving SAT problems, notably resolution, the Davis-Putnam-Logemann-Loveland-Procedure procedure (DPLL) and an extension of it, the conflict-driven clause learning (CDCL). We have formalised these three algorithms in a proof assistant Isabelle/HOL, based on a chapter of ChristophWeidenbach's upcoming book Automed Reasoning – The Art of Generic Problem Solving. The three calculi are presented uniformly as transition systems. We have formally proved that each calculus is a decision procedure for satisfiability of propositional logic. One outcome of the formalization is a verified SAT solver based on DPLL and implemented in a functional programming language.},
author = {Fleury, Mathias},
file = {:Users/jonaprieto/Desktop/Mendeley/2015 - Fleury - Formalisation of Ground Inference Systems in a Proof Assistant.pdf:pdf},
title = {{Formalisation of Ground Inference Systems in a Proof Assistant}},
year = {2015}
}
@article{Kanso2012,
author = {Kanso, Karim},
file = {:Users/jonaprieto/Desktop/Mendeley/2012 - Kanso - Agda as a Platform for the Development of Verified Railway Interlocking Systems.pdf:pdf},
title = {{Agda as a Platform for the Development of Verified Railway Interlocking Systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.310.1502},
year = {2012}
}
@article{Stefanowski2001,
abstract = {The rough set theory, based on the original definition of the indiscernibility relation, is not useful for analysing incomplete information tables where some values of attributes are unknown. In this paper we distinguish two different semantics for incomplete information: the "missing value" semantics and the "absent value" semantics. The already known approaches, e.g. based on the tolerance relations, deal with the missing value case. We introduce two generalisations of the rough sets theory to handle these situations. The first generalisation introduces the use of a non symmetric similarity relation in order to formalise the idea of absent value semantics. The second proposal is based on the use of valued tolerance relations. A logical analysis and the computational experiments show that for the valued tolerance approach it is possible to obtain more informative approximations and decision rules than using the approach based on the simple tolerance relation.},
author = {Stefanowski, J. and Tsouki{\`{a}}s, A.},
doi = {10.1111/0824-7935.00162},
file = {:Users/jonaprieto/Desktop/Mendeley/2001 - Stefanowski, Tsouki{\`{a}}s - Incomplete information tables and rough classification.pdf:pdf},
issn = {08247935},
journal = {Computational Intelligence},
keywords = {Decision rules,Fuzzy sets,Incomplete information,Rough sets,Similarity relation,Valued tolerance relation},
number = {3},
pages = {545--566},
title = {{Incomplete information tables and rough classification}},
volume = {17},
year = {2001}
}
@incollection{Riazanov1999,
abstract = {Vampire is a resolution-based theorem prover for rst-order classical logic. The current version implements ordered binary resolution with the set-of-support strategy and ordered hyperresolution. The competition version will have equality rules.},
address = {Berlin, Heidelberg},
author = {Riazanov, Alexandre and Voronkov, Andrei},
booktitle = {Automated Deduction --- CADE-16: 16th International Conference on Automated Deduction Trento, Italy, July 7--10, 1999 Proceedings},
doi = {10.1007/3-540-48660-7_26},
isbn = {978-3-540-48660-2},
pages = {292--296},
publisher = {Springer Berlin Heidelberg},
title = {{Vampire}},
url = {https://doi.org/10.1007/3-540-48660-7{\_}26},
year = {1999}
}
@article{VanDerWalt2013,
abstract = {This paper explores the recent addition to Agda enablin greflection,in the style of Lisp and Template Haskell. It gives a brief introduction to using reflection, and details the complexities encountered when automating certain proofs with proof by reflection. It presents a library that can be used for automatically quoting a class of concrete Agda terms to a non-dependent, user-defined inductive data type, alleviating some of the burden a programmer faces when using reflection in a practical setting.},
author = {{Van Der Walt}, Paul and Swierstra, Wouter},
doi = {10.1007/978-3-642-41582-1_10},
file = {:Users/jonaprieto/Desktop/Mendeley/2013 - Van Der Walt, Swierstra - Engineering proof by reflection in Agda.pdf:pdf},
isbn = {9783642415814},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Agda,Dependently-typed programming,Metaprogramming,Proof by reflection,Reflection},
pages = {157--173},
title = {{Engineering proof by reflection in Agda}},
volume = {8241 LNCS},
year = {2013}
}
@inproceedings{mohamed2005neural,
author = {Mohamed, Shakir and Marwala, Tshilidzi},
booktitle = {16th Annual Symposium of the Patten Recognition Association of South Africa, Langebaan},
pages = {27--32},
title = {{Neural network based techniques for estimating missing data in databases}},
year = {2005}
}
@incollection{stefanowski1999extension,
author = {Stefanowski, Jerzy and Tsouki{\`{a}}s, Alexis},
booktitle = {New Directions in Rough Sets, Data Mining, and Granular-Soft Computing},
pages = {73--81},
publisher = {Springer},
title = {{On the extension of rough sets under incomplete information}},
year = {1999}
}
@article{Abel2002,
abstract = {We introduce a language based upon lambda calculus with products, coproducts and strictly positive inductive types that allows the definition of recursive terms.  We present the implementation (foetus) of a syntactical check that ensures that all such terms are structurally recursive, i.e. recursive calls appear only with arguments structurally smaller than the input parameters of terms considered.  To ensure the correctness of the termination checker, we show that all structurally recursive terms are normalizing with respect to a given operational semantics.  To this end, we define a semantics on all types and a structural ordering on the values in this semantics and prove that all values are accessible with regard to this ordering.  Finally, we point out how to do this proof predicatively using set based operators.},
author = {Abel, Andreas and Altenkirch, Thorsten},
doi = {10.1017/S0956796801004191},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Abel, Altenkirch - 2002 - A Predicative Analysis of Structural Recursion.pdf:pdf},
issn = {0956-7968},
journal = {Journal of Functional Programming},
number = {01},
pages = {1--41},
title = {{A Predicative Analysis of Structural Recursion}},
url = {http://www.journals.cambridge.org/abstract{\_}S0956796801004191},
volume = {12},
year = {2002}
}
@article{Church1940,
abstract = {Simple type theory is formulated for use with the generic theorem prover Isabelle. This requires explicit type inference rules. There are function, product, and subset types, which may be empty. Descriptions (the eta-operator) introduce the Axiom of Choice. Higher-order logic is obtained through reflection between formulae and terms of type bool. Recursive types and functions can be formally constructed. Isabelle proof procedures are described. The logic appears suitable for general mathematics as well as computational problems.},
archivePrefix = {arXiv},
arxivId = {cs/9301107},
author = {Church, Alonzo},
doi = {10.2307/2266170},
eprint = {9301107},
file = {:Users/jonaprieto/Desktop/Mendeley/1940 - Church - A Formulation of the Simple Theory of Types.pdf:pdf},
issn = {00224812},
journal = {The Journal of Symbolic Logic},
number = {2},
pages = {56--68},
primaryClass = {cs},
title = {{A Formulation of the Simple Theory of Types}},
url = {http://www.jstor.org/stable/2266170},
volume = {5},
year = {1940}
}
@article{Fontaine2006,
abstract = {Formal system development needs expressive specification languages, but also calls for highly automated tools. These two goals are not easy to reconcile, especially if one also aims at high assurances for correctness. In this paper, we describe a combination of {\{}Isabelle/HOL{\}} with a proof-producing {\{}SMT{\}} (Satisfiability Modulo Theories) solver that contains a {\{}SAT{\}} engine and a decision procedure for quantifier-free first-order logic with equality. As a result, a user benefits from the expressiveness of {\{}Isabelle/HOL{\}} when modeling a system, but obtains much better automation for those fragments of the proofs that fall within the scope of the (automatic) {\{}SMT{\}} solver. Soundness is not compromised because all proofs are submitted to the trusted kernel of Isabelle for certification. This architecture is straightforward to extend for other interactive proof assistants and proof-producing reasoners.},
author = {Fontaine, Pascal and Marion, Jean Yves and Merz, Stephan and Nieto, Leonor Prensa and Tiu, Alwen},
doi = {10.1007/11691372_11},
file = {:Users/jonaprieto/Desktop/Mendeley/2006 - Fontaine et al. - Expressiveness automation soundness Towards combining SMT solvers and interactive proof assistants.pdf:pdf},
isbn = {3540330569},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {167--181},
title = {{Expressiveness + automation + soundness: Towards combining SMT solvers and interactive proof assistants}},
volume = {3920 LNCS},
year = {2006}
}
@book{Curry1963,
author = {Curry, Haskell B.},
booktitle = {Journal of the Franklin Institute},
doi = {10.1016/0016-0032(63)90672-0},
file = {:Users/jonaprieto/Desktop/Mendeley/1963 - Curry - Foundations of mathematical logic.pdf:pdf},
isbn = {0486634620},
issn = {00160032},
number = {5},
pages = {438},
pmid = {21192365},
title = {{Foundations of mathematical logic}},
volume = {275},
year = {1963}
}
@article{schafer2002missing,
author = {Schafer, Joseph L and Graham, John W},
journal = {Psychological methods},
number = {2},
pages = {147},
publisher = {American Psychological Association},
title = {{Missing data: our view of the state of the art.}},
volume = {7},
year = {2002}
}
@article{blanchette2013extending,
abstract = {Sledgehammer is a component of Isabelle/HOL that employs resolution-based first-order automatic theorem provers (ATPs) to discharge goals arising in interactive proofs. It heuristically selects relevant facts and, if an ATP is successful, produces a snippet that replays the proof in Isabelle. We extended Sledgehammer to invoke satisfiability modulo theories (SMT) solvers as well, exploiting its relevance filter and parallel architecture. The ATPs and SMT solvers nicely complement each other, and Isabelle users are now pleasantly surprised by SMT proofs for problems beyond the ATPs' reach.},
author = {Blanchette, Jasmin and B{\"{o}}hme, Sascha and Paulson, Lawrence C.},
doi = {10.1007/s10817-013-9278-5},
file = {:Users/jonaprieto/Desktop/Mendeley/2013 - Blanchette, B{\"{o}}hme, Paulson - Extending Sledgehammer with SMT Solvers.pdf:pdf},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
month = {jun},
number = {1},
pages = {109--128},
title = {{Extending Sledgehammer with SMT Solvers}},
url = {https://doi.org/10.1007/s10817-013-9278-5},
volume = {51},
year = {2013}
}
@inproceedings{Sut07-CSR,
author = {Sutcliffe, Geoff},
booktitle = {Proceedings of the 2nd International Computer Science Symposium in Russia},
editor = {Diekert, V and Volkov, M and Voronkov, A},
number = {4649},
pages = {7--23},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{TPTP, TSTP, CASC, etc.}},
year = {2007}
}
@article{kanso2016light,
author = {Kanso, Karim and Setzer, Anton},
doi = {10.1017/S0960129514000140},
file = {:Users/jonaprieto/Desktop/Mendeley/2016 - Kanso, Setzer - A Light-weight Integration Of Automated And Interactive Theorem Proving.pdf:pdf},
issn = {09601295},
journal = {Mathematical Structures in Computer Science},
number = {1},
pages = {129--153},
publisher = {Cambridge University Press},
title = {{A Light-weight Integration Of Automated And Interactive Theorem Proving}},
url = {http://cs.swansea.ac.uk/{~}cskarim/agda/},
volume = {26},
year = {2016}
}
@article{Altenkirch2015,
abstract = {A computer formalisation of the completeness of the boolean model of classical propositional logic is presented. The work follows Huth and Ryan's proof [9]. The proof is constructed for a classical logic system in natural deduction style with all logical connectives. The formalisation is constructive and uses the interactive theorem prover Agda which is an implementation of intensional Martin-L{\"{o}}f type theory [11]. Functions have to be defined in a structurally recursive way to pass the termination checker of Agda. The basic definitions of the formal system must be carefully chosen in order to provide a convenient environment to build correct proofs and meanwhile prevent from getting warnings from the type checker. The formalisation is written in an accessible way so that it can be used for educational purposes. The full source code is available online1. Keywords:},
author = {Cai, Leran and Kaposi, Ambrus and Altenkirch, Thorsten},
file = {:Users/jonaprieto/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Kaposi, Altenkirch - 2015 - Formalising the Completeness Theorem of Classical Propositional Logic in Agda.pdf:pdf},
title = {{Formalising the Completeness Theorem of Classical Propositional Logic in Agda}},
url = {https://akaposi.github.io/proplogic.pdf},
year = {2015}
}
@incollection{Een2004,
author = {E{\'{e}}n, Niklas and S{\"{o}}rensson, Niklas},
doi = {10.1007/978-3-540-24605-3_37},
file = {:Users/jonaprieto/Desktop/Mendeley/2004 - E{\'{e}}n, S{\"{o}}rensson - An Extensible SAT-solver.pdf:pdf},
pages = {502--518},
publisher = {Springer, Berlin, Heidelberg},
title = {{An Extensible SAT-solver}},
url = {http://link.springer.com/10.1007/978-3-540-24605-3{\_}37},
year = {2004}
}
@article{Ge2008,
abstract = {Satisfiability Modulo Theories (SMT) solvers are large and complicated pieces of code. As a result, ensuring their correctness is challenging. In this paper, we discuss a technique for ensuring soundness by producing and checking proofs. We give details of our implementation using CVC3 and HOL Light and provide initial results from our effort to certify the SMT-LIB benchmarks.},
author = {Ge, Yeting and Barrett, Clark},
file = {:Users/jonaprieto/Desktop/Mendeley/2008 - Ge, Barrett - Proof Translation and SMT-LIB Benchmark Certification A Preliminary Report.pdf:pdf},
journal = {Proceedings of the 6{\^{}}{\{}th{\}} International Workshop on Satisfiability Modulo Theories (SMT '08)},
number = {0551645},
pages = {1--11},
title = {{Proof Translation and SMT-LIB Benchmark Certification: A Preliminary Report}},
url = {https://cs.nyu.edu/{~}yeting/prooftrans.pdf},
year = {2008}
}
@article{wang2005classification,
author = {Wang, Shouhong},
journal = {Computers {\&} operations research},
number = {10},
pages = {2583--2594},
publisher = {Elsevier},
title = {{Classification with incomplete survey data: a Hopfield neural network approach}},
volume = {32},
year = {2005}
}
@book{hottbook,
address = {Institute for Advanced Study},
author = {{Univalent Foundations Program}, The},
publisher = {$\backslash$url{\{}https://homotopytypetheory.org/book{\}}},
title = {{Homotopy Type Theory: Univalent Foundations of Mathematics}},
year = {2013}
}
@article{Agudelo-Agudelo2017,
author = {Agudelo-Agudelo, Juan C.},
doi = {10.1007/s11787-017-0168-1},
file = {:Users/jonaprieto/Desktop/Mendeley/2017 - Agudelo-Agudelo - Translating Non-classical Logics into Classical Logic by Using Hidden Variables.pdf:pdf},
issn = {1661-8297},
journal = {Logica Universalis},
number = {2},
pages = {205--224},
title = {{Translating Non-classical Logics into Classical Logic by Using Hidden Variables}},
url = {http://link.springer.com/10.1007/s11787-017-0168-1},
volume = {11},
year = {2017}
}
@misc{Hurd1999,
author = {Hurd, Joe},
doi = {10.1007/3-540-48256-3_21},
file = {:Users/jonaprieto/Desktop/Mendeley/1999 - Hurd - Integrating Gandalf and HOL.pdf:pdf},
pages = {311--321},
publisher = {Springer, Berlin, Heidelberg},
title = {{Integrating Gandalf and HOL}},
url = {http://link.springer.com/10.1007/3-540-48256-3{\_}21},
year = {1999}
}
@article{wang2009discovering,
author = {Wang, Hai and Wang, Shouhong},
journal = {Expert Systems with Applications},
number = {3},
pages = {6256--6260},
publisher = {Elsevier},
title = {{Discovering patterns of missing data in survey databases: an application of rough sets}},
volume = {36},
year = {2009}
}
